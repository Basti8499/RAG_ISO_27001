{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for preprocessing the data before inserting it into the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Helper methods for preprocessing PDF, web and templates files\n",
    "\n",
    "import json\n",
    "from typing import Iterable\n",
    "from langchain.docstore.document import Document\n",
    "from typing import List\n",
    "\n",
    "# Helper methods for storing and loading already generated documents (as the IMG -> TXT process is time-consuming)\n",
    "def store_documents(documents, file_path: str) -> None:\n",
    "    with open(file_path, \"w\") as jsonl_file:\n",
    "        for doc in documents:\n",
    "            jsonl_file.write(doc.json() + \"\\n\")\n",
    "\n",
    "\n",
    "def load_documents(file_path: str) -> List[Document]:\n",
    "    documents = []\n",
    "    with open(file_path, \"r\") as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            data = json.loads(line)\n",
    "            obj = Document(**data)\n",
    "            documents.append(obj)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AsyncHtmlLoader\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "import pypdfium2 as pdfium\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import pytesseract\n",
    "\n",
    "pytesseract_path = os.environ.get(\"TESSERACT_PATH\")\n",
    "pytesseract.pytesseract.tesseract_cmd = pytesseract_path\n",
    "\n",
    "\n",
    "def update_pdf_documents() -> List[Document]:\n",
    "    \"\"\"\n",
    "    Method for processing and updating documents based on the PDFs. For that the PDFs, that were not processed yet, are converted to images and then transformed to texts.\n",
    "    For each PDF one document is then created with all text from all pages. In the end the filename is changed, so that it is clear that it was already processed.\n",
    "    This approach is used because different methods were tested (DirectoryLoader, PyPDFLoader, UnstructuredPDFLoader, PyPDFium2Loader, PDFMinerLoader, PyMuPDFLoader, PDFPlumberLoader) but the PDF -> IMG + IMG -> TXT approach was the best performing one, creating nicely structured documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # List for either all documents or only new ones\n",
    "    documents_PDF = []\n",
    "    # List for all documents\n",
    "    already_processed_documents = load_documents(\"./../../inputData/PDF/documents/all_documents\")\n",
    "\n",
    "    PDF_images_path = \"./../../inputData/PDF/PDF_Images\"\n",
    "    directory_path = \"./../../inputData/PDF/cleaned\"\n",
    "\n",
    "    # Go through each PDF file in the directory\n",
    "    for file in os.listdir(directory_path):\n",
    "        if \"Tesseract_processed\" not in file:\n",
    "            file_path = os.path.join(directory_path, file)\n",
    "            pdf = pdfium.PdfDocument(file_path)\n",
    "            n_pages = len(pdf)\n",
    "            # Create directory to store the image\n",
    "            os.makedirs(PDF_images_path + f\"/{file}\")\n",
    "            complete_text = \"\"\n",
    "            # Go through each page of the PDF and save the according image\n",
    "            for page_number in range(n_pages):\n",
    "                page = pdf.get_page(page_number)\n",
    "                pil_image = page.render(\n",
    "                    scale=300 / 72,\n",
    "                    rotation=0,\n",
    "                    crop=(0, 0, 0, 0),\n",
    "                ).to_pil()\n",
    "                pil_image_path = PDF_images_path + f\"/{file}/image_{page_number+1}.png\"\n",
    "                pil_image.save(pil_image_path)\n",
    "                img = cv2.imread(pil_image_path)\n",
    "                # Convert image to grayscale\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                # Apply threshold to convert to binary image\n",
    "                threshold_img = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "                # Pass the image through pytesseract and add the text to the whole document text\n",
    "                complete_text += pytesseract.image_to_string(threshold_img) + \"\\n\"\n",
    "                # Remove the image as it is already processed\n",
    "                os.remove(pil_image_path)\n",
    "\n",
    "                file_name_without_pdf = file\n",
    "                if file.endswith(\".pdf\"):\n",
    "                    file_name_without_pdf = file[:-4]\n",
    "            # Create a document based on the whole text and metadata\n",
    "            document_PDF = Document(page_content=complete_text, metadata={\"source\": file, \"title\": file_name_without_pdf})\n",
    "            documents_PDF.append(document_PDF)\n",
    "            already_processed_documents.append(document_PDF)\n",
    "\n",
    "            # Change the filename, so that in future calls the PDF is not processed again\n",
    "            new_filename = file.replace(\".pdf\", \"_Tesseract_processed.pdf\")\n",
    "            new_pdf_path = os.path.join(directory_path, new_filename)\n",
    "            print(new_pdf_path)\n",
    "            pdf.close()\n",
    "            os.rename(file_path, new_pdf_path)\n",
    "\n",
    "    # Store docs if new documents were processed\n",
    "    if len(documents_PDF) > 0:\n",
    "        # Store all documents, including the new ones\n",
    "        store_documents(already_processed_documents, \"./../../inputData/PDF/documents/all_documents\")\n",
    "        # Store the new documents\n",
    "        store_documents(documents_PDF, \"./../../inputData/PDF/documents/new_documents\")\n",
    "    \n",
    "    # Delete the empty folders inside the images folder\n",
    "    target_dir = \"./../input_data/PDF/PDF_images\"\n",
    "\n",
    "    # Check if the target directory exists to avoid errors\n",
    "    if os.path.exists(target_dir):\n",
    "        # List all the items in the directory\n",
    "        for item in os.listdir(target_dir):\n",
    "            item_path = os.path.join(target_dir, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                # Use shutil.rmtree to delete the directory and all its contents\n",
    "                shutil.rmtree(item_path)\n",
    "\n",
    "def get_web_documents_for_cleaning() -> List[Document]:\n",
    "    \"\"\"\n",
    "    Method for returning documents based on the URLs. Different methods were tested, but the combination of AsyncHTML and HTML2Text produced best results (very structured text).\n",
    "    \"\"\"\n",
    "    directory_path_web = \"./../../inputData/Web/URLs/uncleaned_urls.txt\"\n",
    "\n",
    "    imported_urls = []\n",
    "    with open(directory_path_web, \"r\") as file:\n",
    "        for line in file:\n",
    "            imported_urls.append(line.strip())\n",
    "\n",
    "    loader_web = AsyncHtmlLoader(imported_urls)\n",
    "    documents_web = loader_web.load()\n",
    "\n",
    "    html2text = Html2TextTransformer()\n",
    "    documents_web_transformed = html2text.transform_documents(documents_web)\n",
    "    print(\"Number of documents: \" + str(len(documents_web_transformed)) + \"\\n\")\n",
    "\n",
    "    return documents_web_transformed\n",
    "\n",
    "\n",
    "def get_pdf_documents(all_docs: bool):\n",
    "    \"\"\"\n",
    "    Method for returning the documents of the PDFs. Processing and updating takes place in update_pdf_documents.\n",
    "    all_docs parameter defines whether to load all documents or only new ones. Only new ones can be used if the index is already build and new documents should be added.\n",
    "    \"\"\"\n",
    "    pdf_documents = []\n",
    "    if all_docs:\n",
    "        pdf_documents = load_documents(\"./../../inputData/PDF/documents/all_documents\")\n",
    "    else:\n",
    "        pdf_documents = load_documents(\"./../../inputData/PDF/documents/new_documents\")\n",
    "\n",
    "    return pdf_documents\n",
    "\n",
    "\n",
    "def get_web_documents(all_docs: bool) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Method for returning the already processed documents. FIRST need to call get_web_docs_for_cleaning and clean manually. As it is a manual cleaning process, the methods are need to be called asynchronously.\n",
    "    \"\"\"\n",
    "    web_documents = []\n",
    "    if all_docs:\n",
    "        web_documents = load_documents(\"./../../inputData/Web/documents/all_cleaned_documents\")\n",
    "    else:\n",
    "        web_documents = load_documents(\"./../../inputData/Web/documents/newly_cleaned_documents\")\n",
    "\n",
    "    return web_documents\n",
    "\n",
    "\n",
    "def get_template_documents(all_docs: bool) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Method for returning the documents of the templates.\n",
    "    \"\"\"\n",
    "    template_documents = []\n",
    "    if all_docs:\n",
    "        template_documents = load_documents(\"./../../inputData/Templates/documents/all_documents\")\n",
    "    else:\n",
    "        template_documents = load_documents(\"./../../inputData/Templates/documents/new_documents\")\n",
    "\n",
    "    return template_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and execute if PDF documents should be updated.\n",
    "#update_pdf_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import/read input data (PDF, Web, Template, PDF_Web or All)\n",
    "def get_documents_from_files(file_type: str, all_docs: bool):\n",
    "    \"\"\"\n",
    "    Gets the specified documents for file type and all docs.\n",
    "    \"\"\"\n",
    "    if file_type == \"PDF\":\n",
    "        return get_pdf_documents(all_docs)\n",
    "    elif file_type == \"Web\":\n",
    "        return get_web_documents(all_docs)\n",
    "    elif file_type == \"Template\":\n",
    "        return get_template_documents(all_docs)\n",
    "    elif file_type == \"PDF_Web\":\n",
    "        documents_PDF = get_pdf_documents(all_docs)\n",
    "        document_web = get_web_documents(all_docs)\n",
    "        document_web.extend(documents_PDF)\n",
    "        print(\"Number of documents: \" + str(len(document_web)) + \"\\n\")\n",
    "        return document_web\n",
    "    elif file_type == \"All\":\n",
    "        documents_PDF = get_pdf_documents(all_docs)\n",
    "        document_web = get_web_documents(all_docs)\n",
    "        document_template = get_template_documents(all_docs)\n",
    "        document_web.extend(documents_PDF)\n",
    "        document_web.extend(document_template)\n",
    "        print(\"Number of documents: \" + str(len(document_web)) + \"\\n\")\n",
    "        return document_web\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Error, raised exception: Wrong fileType provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Chunk data\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_docs(documents: List[Document], chunk_size: int, chunk_overlap: int):\n",
    "    \"\"\"\n",
    "    Splits the docs into chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, separators=[\" \"])\n",
    "    chunkedDocuments = text_splitter.split_documents(documents)\n",
    "    return chunkedDocuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Clean chunked data\n",
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # Replace multiple whitespaces (except newlines) with a single space\n",
    "    text = re.sub(r\"(?!\\n)\\s+\", \" \", text)\n",
    "    # Replace multiple newlines with a single newline\n",
    "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
    "    # Remove leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def clean_and_process_chunked_documents(chunkedDocuments: List[Document], append_summaries_to_each_doc: bool) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Cleans, lowercases and appends summaries to the documents if wanted.\n",
    "    \"\"\"\n",
    "\n",
    "    summaries = []\n",
    "    if append_summaries_to_each_doc:\n",
    "        # Change path if needed!\n",
    "        with open(\"./../../evaluationInput/retrieval_eval/summaries_1536_264.json\", 'r') as json_file:\n",
    "            summaries = json.load(json_file)\n",
    "\n",
    "    # Clean whitespaces, add capitalized original text as metadata, lowercase content and if wanted add title to the document\n",
    "    counter = 1\n",
    "\n",
    "    if append_summaries_to_each_doc and len(summaries) == len(chunkedDocuments):\n",
    "       \n",
    "        previous_summary = None  # Initialize a variable to store the summary of the previous document\n",
    "        previous_source = None  # Initialize a variable to store the title of the previous document\n",
    "\n",
    "        for i, summary in zip(chunkedDocuments, summaries):\n",
    "            i.page_content = clean_text(i.page_content)\n",
    "            i.metadata[\"original_text\"] = i.page_content\n",
    "            i.metadata[\"doc_ID\"] = counter\n",
    "            counter += 1\n",
    "\n",
    "            current_source = i.metadata[\"source\"]\n",
    "\n",
    "            # Check if the current document's title is the same as the previous document's title\n",
    "            if previous_source is not None and current_source == previous_source:\n",
    "                # If titles match, and there is a summary from the previous document, prepend it\n",
    "                if previous_summary is not None:\n",
    "                    i.page_content = previous_summary + \"\\n\" + i.page_content\n",
    "            \n",
    "            # Update previous_summary and previous_title for the next iteration\n",
    "            previous_summary = summary\n",
    "            previous_source = current_source\n",
    "\n",
    "            i.page_content = i.page_content.lower()\n",
    "    else:\n",
    "         for i in chunkedDocuments:\n",
    "            i.page_content = clean_text(i.page_content)\n",
    "            i.metadata[\"original_text\"] = i.page_content\n",
    "            i.metadata[\"doc_ID\"] = counter\n",
    "            counter += 1\n",
    "\n",
    "            i.page_content = i.page_content.lower()       \n",
    "\n",
    "    return chunkedDocuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import CohereEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import VoyageEmbeddings\n",
    "\n",
    "def create_embedding_model(model_provider: str, model_name: str):\n",
    "    \"\"\"\n",
    "    Creates the embedding model and returns it. Possible combinations are (modelProvider: modelName (embedding size, max input length))\n",
    "    - Cohere: v2 (4096, 512) | v3 (1024, 512) [https://docs.cohere.com/reference/embed]\n",
    "    - OpenAI: text-embedding-ada-002 (1536, 8191) [https://platform.openai.com/docs/guides/embeddings]\n",
    "    - Voyage: voyage-lite-01 (1024, 4096) [https://docs.voyageai.com/embeddings/]\n",
    "    - HuggingFace: \n",
    "        - all-mpnet-base-v2 (768, 384) [https://huggingface.co/sentence-transformers/all-mpnet-base-v2]\n",
    "        - all-MiniLM-L6-v2 (384, 256) [https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2]\n",
    "        - bge-large-en-v1.5 (1024, 512) [https://huggingface.co/BAAI/bge-large-en-v1.5]\n",
    "        - SecRoBERTa (824, 512) [https://huggingface.co/jackaduma/SecRoBERTa]\n",
    "    - Fine-tuned:\n",
    "        - finetuned-ISO-27001_1024 (1024, 512) [https://huggingface.co/Basti8499/bge-large-en-v1.5-ISO-27001]\n",
    "    \"\"\"\n",
    "\n",
    "    if model_provider == \"Cohere\":\n",
    "        if model_name == \"v2\":\n",
    "            embeddings = CohereEmbeddings(model=\"embed-english-v2.0\")\n",
    "            print(\"Cohere v2 embedding: Vector embedding size - 4096, input length: 512\")\n",
    "            return embeddings\n",
    "        if model_name == \"v3\":\n",
    "            embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\n",
    "            print(\"Cohere v3 embedding: Vector embedding size - 1024, input length: 512\")\n",
    "            return embeddings\n",
    "\n",
    "    elif model_provider == \"HuggingFace\":\n",
    "        if model_name == \"all-mpnet-base-v2\":\n",
    "            embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "            print(\"HuggingFace all-mpnet-base-v2 embedding - Vector embedding size: 768, input length: 384\")\n",
    "            return embeddings\n",
    "        if model_name == \"all-MiniLM-L6-v2\":\n",
    "            embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "            print(\"HuggingFace all-MiniLM-L6-v2 embedding - Vector embedding size: 384, input length: 256\")\n",
    "            return embeddings\n",
    "        if model_name == \"bge-large-en-v1.5\":\n",
    "            embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "            print(\"HuggingFace BAAI/bge-large-en-v1.5 embedding - Vector embedding size: 1024, input length: 512\")        \n",
    "            return embeddings\n",
    "        if model_name == \"Contriever\":\n",
    "            embeddings = HuggingFaceEmbeddings(model_name = \"facebook/contriever-msmarco\")\n",
    "            print(\"HuggingFace facebook/contriever-msmarco embedding - Vector embedding size: 768, input length: 512\")   \n",
    "            return embeddings     \n",
    "        if model_name == \"SecRoBERTa\":\n",
    "            embeddings = HuggingFaceEmbeddings(model_name=\"jackaduma/SecRoBERTa\")\n",
    "            print(\"HuggingFace jackaduma/SecRoBERTa embedding - Vector embedding size: 768, input length: 512\")        \n",
    "            return embeddings\n",
    "        \n",
    "    elif model_provider == \"Voyage\":\n",
    "        if model_name == \"voyage-2\":\n",
    "            embeddings = VoyageEmbeddings(model=\"voyage-2\", show_progress_bar=True, batch_size=200)\n",
    "            print(\"Voyage embedding - Vector embedding size: 1024, input length: 4096\")\n",
    "            return embeddings \n",
    "        \n",
    "    elif model_provider == \"OpenAI\":\n",
    "        if model_name == \"text-embedding-ada\":\n",
    "            embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "            print(\"OpenAI embedding - Vector embedding size: 1536, input length: 8191\")\n",
    "            print(\"Tokenizer used: cl100k_base\")\n",
    "            return embeddings\n",
    "        \n",
    "    elif model_provider == \"Fine-tuned\":\n",
    "        if model_name == \"finetuned-ISO-27001_1024\":\n",
    "            embeddings = HuggingFaceEmbeddings(model_name=\"Basti8499/bge-large-en-v1.5-ISO-27001\")\n",
    "            print(\"Fine-tuned bge-large-en-v1.5 with ISO 27001 - Vector embedding size: 1024, input length: 512\")   \n",
    "            return embeddings                 \n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"Error, raised exception: Wrong modelProvider or modelName provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_vectors(embedding_model, documents: List[Document]):\n",
    "    \"\"\"\n",
    "    Creates the embeddings from the documents.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for document in documents:\n",
    "        texts.append(document.page_content)\n",
    "\n",
    "    embeddings = embedding_model.embed_documents(texts)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    model_provider: str,\n",
    "    model_name: str,\n",
    "    file_type: str,\n",
    "    append_title_to_each_doc: bool,\n",
    "    all_docs: bool,\n",
    "    with_Kersten: bool = False,\n",
    "    is_recursive: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Puts all the above methods together and preprocesses the data for later indexing.\n",
    "    \"\"\"\n",
    "    print(\"Starting to preprocess data for: Chunk Size - \" + str(chunk_size) + \", Chunk Overlap - \" + str(chunk_overlap) + \", Model Name: \" + model_name)\n",
    "    documents = get_documents_from_files(file_type, all_docs)\n",
    "    chunked_documents = split_docs(documents, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunked_cleaned_documents = clean_and_process_chunked_documents(chunked_documents, append_title_to_each_doc)\n",
    "    embedding_model = create_embedding_model(model_provider, model_name)\n",
    "    embeddings = create_embedding_vectors(embedding_model, chunked_cleaned_documents)\n",
    "\n",
    "    return chunked_cleaned_documents, embedding_model, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def preprocess_data_for_parent_child_retriever(\n",
    "    chunk_size_parent: int,\n",
    "    chunk_overlap_parent: int,\n",
    "    chunk_size_child: int,\n",
    "    chunk_overlap_child: int,\n",
    "    model_provider: str,\n",
    "    model_name: str,\n",
    "    file_type: str,\n",
    "    append_title_to_each_doc: bool,\n",
    "    all_docs: bool,\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocesses the data in the case of hierarchical retrieval with the above methods.\n",
    "    \"\"\"\n",
    "    # First process the documents as in the standard case\n",
    "    documents = get_documents_from_files(file_type, all_docs)\n",
    "    parent_documents = split_docs(documents, chunk_size=chunk_size_parent, chunk_overlap=chunk_overlap_parent)\n",
    "    parent_cleaned_documents = clean_and_process_chunked_documents(parent_documents, append_title_to_each_doc)\n",
    "    # Generate an ID for each document, so in later retrieval the parent doc can be retrieved over the child doc\n",
    "    parent_doc_ids = [str(uuid.uuid4()) for _ in parent_cleaned_documents]\n",
    "\n",
    "    child_doc_list = []\n",
    "    parent_full_docs = []\n",
    "    index = 0\n",
    "    for parent_doc in parent_cleaned_documents:\n",
    "        \n",
    "        parent_id = parent_doc_ids[index]\n",
    "        index +=1\n",
    "        # Append a tuple of an ID and the according document (fs store needs this format)\n",
    "        parent_full_docs.append((parent_id, parent_doc))\n",
    "        parent_doc = [parent_doc]\n",
    "\n",
    "        child_documents = split_docs(parent_doc, chunk_size=chunk_size_child, chunk_overlap=chunk_overlap_child)\n",
    "        for child_doc in child_documents:\n",
    "            # Set the parent_id for the parent document in the metadata of the child\n",
    "            child_doc.metadata[\"parent_id\"] = parent_id\n",
    "        child_doc_list.extend(child_documents)\n",
    "\n",
    "    embedding_model = create_embedding_model(model_provider, model_name)    \n",
    "    child_embeddings = create_embedding_vectors(embedding_model, child_doc_list)\n",
    "\n",
    "    return parent_full_docs, child_doc_list, child_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_documents_for_sparse_retrieval(chunk_size: int, chunk_overlap: int, file_type: str, append_title_to_each_doc: bool):\n",
    "    \"\"\"\n",
    "    Method used for storing the documents for sparse retrieval in the local memory.\n",
    "    \"\"\"\n",
    "    documents = get_documents_from_files(file_type, True)\n",
    "    chunked_documents = split_docs(documents, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunked_cleaned_documents = clean_and_process_chunked_documents(chunked_documents, append_title_to_each_doc)\n",
    "\n",
    "    document_file_name = str(chunk_size) + \"_\" + str(chunk_overlap) + \"_\" + file_type + \"_\" + str(append_title_to_each_doc)\n",
    "    store_documents(chunked_cleaned_documents, f\"./../../retrievalInput/Documents_For_Sparse/{document_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage._lc_store import create_kv_docstore\n",
    "from langchain.storage.file_system import LocalFileStore\n",
    "\n",
    "def store_documents_for_sparse_retrieval_parent_child(\n",
    "    chunk_size_parent: int,\n",
    "    chunk_overlap_parent: int,\n",
    "    chunk_size_child: int,\n",
    "    chunk_overlap_child: int,\n",
    "    file_type: str,\n",
    "    append_title_to_each_doc: bool):\n",
    "\n",
    "    \"\"\"\n",
    "    Method used for storing the documents for sparse retrieval in the case of hierarchical retrieval in the local memory.\n",
    "    \"\"\"\n",
    "\n",
    "    documents = get_documents_from_files(file_type, True)\n",
    "    parent_documents = split_docs(documents, chunk_size=chunk_size_parent, chunk_overlap=chunk_overlap_parent)\n",
    "    parent_cleaned_documents = clean_and_process_chunked_documents(parent_documents, append_title_to_each_doc)\n",
    "    # Generate an ID for each document, so in later retrieval the parent doc can be retrieved over the child doc\n",
    "    parent_doc_ids = [str(uuid.uuid4()) for _ in parent_cleaned_documents]\n",
    "\n",
    "    child_doc_list = []\n",
    "    parent_full_docs = []\n",
    "    index = 0\n",
    "    \n",
    "    for parent_doc in parent_cleaned_documents:\n",
    "        \n",
    "        parent_id = parent_doc_ids[index]\n",
    "        index +=1\n",
    "        # Append a tuple of an ID and the according document (fs store needs this format)\n",
    "        parent_full_docs.append((parent_id, parent_doc))\n",
    "        parent_doc = [parent_doc] # split_docs can only process arrays\n",
    "\n",
    "        child_documents = split_docs(parent_doc, chunk_size=chunk_size_child, chunk_overlap=chunk_overlap_child)\n",
    "        for child_doc in child_documents:\n",
    "            # Set the parent_id for the parent document in the metadata of the child\n",
    "            child_doc.metadata[\"parent_id\"] = parent_id\n",
    "        child_doc_list.extend(child_documents)\n",
    "\n",
    "    document_file_name = str(chunk_size_parent) + \"_\" + str(chunk_overlap_parent) + \"_PC_\" + str (chunk_size_child) + \"_\" + str(chunk_overlap_child) + \"_\" + file_type + \"_\" + str(append_title_to_each_doc)\n",
    "    store_documents(child_doc_list, f\"./../../retrievalInput/Documents_For_Sparse/{document_file_name}\")\n",
    "\n",
    "    fs = LocalFileStore(os.environ.get(\"PARENT_DOC_PATH\") + f\"\\\\{document_file_name}\")\n",
    "    store = create_kv_docstore(fs)\n",
    "    store.mset(parent_full_docs)\n",
    "    print(\"Successfully created local file store for parent docs. There are\", len(parent_documents), \"parent documents in the file store.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
