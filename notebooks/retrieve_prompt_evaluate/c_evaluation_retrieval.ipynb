{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for evaluating the retrieval. Both, generates the evaluation datasets and provides method for evaluating.>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "def load_question_context_pairs(chunk_size: str, chunk_overlap: str, file_type: str, append_title: str) -> object:\n",
    "    \"\"\"\n",
    "    Loads already created QC pairs for evaluation.\n",
    "    \"\"\"\n",
    "    qc_file_name = \"QC_\" + chunk_size + \"_\" + chunk_overlap + \"_\" + file_type + \"_\" + append_title + \".json\"\n",
    "    file_path = f\"./../../evaluationInput/retrieval_eval/{qc_file_name}\"\n",
    "\n",
    "    try:\n",
    "        # Open the file in read mode ('r')\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            # Load the JSON content from the file\n",
    "            data = json.load(json_file)\n",
    "            return data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # Handle the case where the file doesn't exist\n",
    "        print(f\"The file {file_path} does not exist.\")\n",
    "        return []\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        # Handle the case where the file is not valid JSON\n",
    "        print(f\"Error decoding JSON in {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def store_question_context_pairs(question_context_pairs: object, file_path: str):\n",
    "    \"\"\"\n",
    "    Stores the created QC pairs for later evaluation.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(question_context_pairs, json_file)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from ipynb.fs.defs.a_setup_llms import setup_llm\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "\n",
    "def generate_question(input_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses GPT 3.5 Turbo to generate a question for a given context.\n",
    "    \"\"\"\n",
    "\n",
    "    llm, token_size = setup_llm(\"OpenAI\", \"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
    "        \"\"\"You are a Professor. Your task is to setup exactly one question based on the provided context for an upcoming quiz/examination. The question should not contain options and not start with Q1. Restrict the questions to the context information provided and just return the question, never return an introduction or something prior to the question e.g. 'Question :'. In addition, never mention that the question is based on the context or \"given context\"\"\"\n",
    "    )\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(\"\"\"Context information: {input_text}\"\"\")\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "    generated_string = llm(chat_prompt.format_prompt(input_text=input_text).to_messages()).content\n",
    "\n",
    "    # Clean the string, so only the question is extracted\n",
    "    string_lines = str(generated_string).strip().split(\"\\n\")\n",
    "    # Remove numbering from lines\n",
    "    questions = [re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in string_lines]\n",
    "    # Only include lines with question mark\n",
    "    questions = [question for question in questions if len(question) > 0 and \"?\" in question]\n",
    "\n",
    "    if len(questions) == 1:\n",
    "        return questions[0]\n",
    "    else:\n",
    "        return generated_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "import os\n",
    "import random\n",
    "from langchain.storage.file_system import LocalFileStore\n",
    "from langchain.storage._lc_store import create_kv_docstore\n",
    "\n",
    "\n",
    "def generate_and_store_question_context_pairs(collection_names: List[str], k_total: int):\n",
    "    \"\"\"\n",
    "    Generates k question context pairs for each collection name given. Stores it in the directory.\n",
    "\n",
    "    Data format: {\"collection_name\": \"xyz\", \"question_context_pairs\": [{\"question\": \"xyz\", \"context\": \"xyz\", \"context_id\": 1}, {\"question\": \"xyz\", \"context\": \"xyz\", \"context_id\": 2}]}\n",
    "    \"\"\"\n",
    "    new_client = chromadb.PersistentClient(path=os.environ.get(\"CHROMA_PATH\"))\n",
    "\n",
    "    # For each collection create question context pairs\n",
    "    for collection_name in collection_names:\n",
    "        print(\"Starting to generate QC pairs for \" + collection_name)\n",
    "        try:\n",
    "            new_client.get_collection(collection_name)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: Collection {collection_name} does not exist. Will be skipped.\")\n",
    "            continue\n",
    "\n",
    "        vectordb = Chroma(client=new_client, collection_name=collection_name)\n",
    "        if \"PC_\" in collection_name:\n",
    "            metadata = {\n",
    "                \"file_type\": vectordb._collection.metadata[\"file_type\"],\n",
    "                \"chunk_size\": vectordb._collection.metadata[\"chunk_size_parent\"],\n",
    "                \"chunk_overlap\": vectordb._collection.metadata[\"chunk_overlap_parent\"],\n",
    "                \"title_appended\": vectordb._collection.metadata[\"title_appended\"],\n",
    "            }\n",
    "        else:\n",
    "            metadata = {\n",
    "                \"file_type\": vectordb._collection.metadata[\"file_type\"],\n",
    "                \"chunk_size\": vectordb._collection.metadata[\"chunk_size\"],\n",
    "                \"chunk_overlap\": vectordb._collection.metadata[\"chunk_overlap\"],\n",
    "                \"title_appended\": vectordb._collection.metadata[\"title_appended\"],\n",
    "            }\n",
    "\n",
    "        # Try to load QC pairs based on the same metadata.\n",
    "        existing_q_c_object = load_question_context_pairs(metadata[\"chunk_size\"], metadata[\"chunk_overlap\"], metadata[\"file_type\"], metadata[\"title_appended\"])\n",
    "        existing_q_c_pairs = []\n",
    "\n",
    "        if len(existing_q_c_object) > 0:\n",
    "            existing_q_c_pairs = existing_q_c_object[\"question_context_pairs\"]\n",
    "\n",
    "        # Get the IDs for either the filestore, in the case of hierarchical retrieval, or for the vector database. \n",
    "        if \"PC_\" in collection_name:\n",
    "            fs = LocalFileStore(os.environ.get(\"PARENT_DOC_PATH\") + f\"\\\\{collection_name}\")\n",
    "            store = create_kv_docstore(fs)\n",
    "            existing_ids = []\n",
    "            for key in store.yield_keys():\n",
    "                existing_ids.append(key)\n",
    "        else:\n",
    "            existing_ids = vectordb.get()[\"ids\"]\n",
    "\n",
    "        # Get the total document count, in order to randomly generate ids and check how many q,c pairs need to be generated\n",
    "        document_count = len(existing_ids)\n",
    "        pairs_to_generate = 0\n",
    "        q_c_list = []\n",
    "\n",
    "        # If q_c pairs for the combination of the index metadata already exist and the size is already larger than k, return\n",
    "        if len(existing_q_c_pairs) > 0 and len(existing_q_c_pairs) >= k_total:\n",
    "            return\n",
    "        # If q_c pairs for the combination of the index metadata already exist and the size is smaller than k\n",
    "        elif len(existing_q_c_pairs) > 0 and len(existing_q_c_pairs) < k_total:\n",
    "            difference = k_total - len(existing_q_c_pairs)\n",
    "            # and the difference is larger than the document count, return\n",
    "            if difference > document_count:\n",
    "                return\n",
    "            # and the difference is smaller than the document count, generate the difference in q,c pairs and store both new, and old pairs\n",
    "            else:\n",
    "                pairs_to_generate = difference\n",
    "                q_c_list.extend(existing_q_c_pairs)\n",
    "        # If q_c pairs for the combination of the index metadata do not exist and the document count inside the index are >= than k, generate k q,c pairs\n",
    "        elif len(existing_q_c_pairs) == 0 and document_count >= k_total:\n",
    "            pairs_to_generate = k_total\n",
    "        # If q_c pairs for the combination of the index metadata do not exist and the document count inside the index is < than k, generate q,c pairs for all documents\n",
    "        elif len(existing_q_c_pairs) == 0 and document_count < k_total:\n",
    "            print(\"Test Case 2\")\n",
    "            pairs_to_generate = document_count\n",
    "\n",
    "        print(\"Number of pairs to generate: \" + str(pairs_to_generate))\n",
    "\n",
    "        # Sample random ids and get the corresponding documents\n",
    "        random_ids = random.sample(existing_ids, pairs_to_generate)\n",
    "        if \"PC_\" in collection_name:\n",
    "            documents_from_store = store.mget(random_ids)\n",
    "            metadata_docs = [doc.metadata for doc in documents_from_store]\n",
    "            id_docs = [doc.metadata[\"doc_ID\"] for doc in documents_from_store]\n",
    "            chosen_documents = {\"ids\": id_docs, \"metadatas\": metadata_docs}\n",
    "        else:\n",
    "            chosen_documents = vectordb._collection.get(ids=random_ids)\n",
    "\n",
    "        # Iterate over these documents and generate a question based on the document. Append the resulting object to the whole list of all question, context pairs\n",
    "        for id_value, metadata_value in zip(chosen_documents[\"ids\"], chosen_documents[\"metadatas\"]):\n",
    "\n",
    "            document_original_text = metadata_value[\"original_text\"]\n",
    "\n",
    "            question = generate_question(document_original_text)\n",
    "\n",
    "            q_c_pair = {\"question\": question, \"context\": document_original_text, \"context_id\": id_value}\n",
    "            q_c_list.append(q_c_pair)\n",
    "\n",
    "        q_c_object = {\n",
    "            \"collection_name\": metadata[\"chunk_size\"] + \"_\" + metadata[\"chunk_overlap\"] + \"_\" + metadata[\"file_type\"] + \"_\" + metadata[\"title_appended\"],\n",
    "            \"question_context_pairs\": q_c_list,\n",
    "        }\n",
    "\n",
    "        qc_file_name = (\n",
    "            \"QC_\" + metadata[\"chunk_size\"] + \"_\" + metadata[\"chunk_overlap\"] + \"_\" + metadata[\"file_type\"] + \"_\" + metadata[\"title_appended\"] + \".json\"\n",
    "        )\n",
    "        file_path = f\"./../../evaluationInput/retrieval_eval/{qc_file_name}\"\n",
    "        print(\"Stored QC file name: \" + qc_file_name)\n",
    "\n",
    "        store_question_context_pairs(q_c_object, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "def calculate_hit_rate(retrieved_docs: List[Document], q_c_pair: object) -> float:\n",
    "    '''\n",
    "    Caluclates the hit rate for one QC pair.\n",
    "    '''\n",
    "    expected_id = q_c_pair[\"context_id\"]\n",
    "\n",
    "    is_hit = any(str(doc.metadata[\"doc_ID\"]) == expected_id or doc.metadata[\"doc_ID\"] == expected_id for doc in retrieved_docs)\n",
    "    return 1.0 if is_hit else 0.0\n",
    "\n",
    "def calculate_average_hit_rate(retrieved_docs_for_pairs: List[List[Document]], q_c_pairs: List[object]) -> float:\n",
    "    '''\n",
    "    Calculates the average hit rate over all QC pairs.\n",
    "    Hit Rate = Is the expected ID inside the retrieved docs? Then return 1, otherwise 0\n",
    "    '''\n",
    "\n",
    "    if len(retrieved_docs_for_pairs) != len(q_c_pairs):\n",
    "        raise Exception(\"Error at calculating average hit rate, raised exception: Different length of retrieved_docs_for_pairs and q_c_pairs\")\n",
    "    \n",
    "    hit_rates = []\n",
    "    for retrieved_doc_list, q_c_pair in zip(retrieved_docs_for_pairs, q_c_pairs):\n",
    "        hit_rates.append(calculate_hit_rate(retrieved_doc_list, q_c_pair))\n",
    "\n",
    "    average_hit_rate = sum(hit_rates) / len(hit_rates)\n",
    "\n",
    "    return average_hit_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(retrieved_docs: List[Document], q_c_pair: object) -> float:\n",
    "    '''\n",
    "    Caluclates the hit mean reciprocal rank for one QC pair.\n",
    "    '''\n",
    "    expected_id = q_c_pair[\"context_id\"]\n",
    "    for index, doc in enumerate(retrieved_docs):\n",
    "        if str(doc.metadata[\"doc_ID\"]) == expected_id or doc.metadata[\"doc_ID\"] == expected_id:\n",
    "            mrr = 1 / (index + 1)\n",
    "            return mrr\n",
    "\n",
    "    return 0.0\n",
    "\n",
    "def calculate_average_mrr(retrieved_docs_for_pairs: List[List[Document]], q_c_pairs: List[object]) -> float:\n",
    "    '''\n",
    "    Calculates the average MRR over all QC pairs.\n",
    "    \n",
    "    Mean Reciprocal Rank = Is the expected ID inside the retrieved docs and on which index? Based on the index return 1/(index+1).\n",
    "    So if the retrieved doc was the first one (the most relevant one), return 1.\n",
    "    '''\n",
    "    if len(retrieved_docs_for_pairs) != len(q_c_pairs):\n",
    "        raise Exception(\"Error at calculating average mrr, raised exception: Different length of retrieved_docs_for_pairs and q_c_pairs\")\n",
    "    \n",
    "    mrrs = []\n",
    "    for retrieved_doc_list, q_c_pair in zip(retrieved_docs_for_pairs, q_c_pairs):\n",
    "        mrrs.append(calculate_mrr(retrieved_doc_list, q_c_pair))\n",
    "\n",
    "    average_mrr = sum(mrrs) / len(mrrs)\n",
    "\n",
    "    return average_mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_time(durations: List[float]) -> float:\n",
    "    return sum(durations)/len(durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagReranker\n",
    "\n",
    "def calculate_average_similarity_score_BGE(retrieved_docs_for_pairs: List[List[Document]], q_c_pairs: List[object], chunk_size) -> float:\n",
    "    \"\"\"\n",
    "    Calculates how relevant the retrieved k documents are for the given question. Uses the bge-reranker-base cross-encoder.\n",
    "    The score is calculated for each of the k retrieved documents and then averaged. The overall score is once again the average over all pairs in the dataset\n",
    "    \"\"\"\n",
    "    if len(retrieved_docs_for_pairs) != len(q_c_pairs):\n",
    "        raise Exception(\"Error at calculating average similarity score, raised exception: Different length of retrieved_docs_for_pairs and q_c_pairs\")\n",
    "    \n",
    "    reranker = FlagReranker('BAAI/bge-reranker-base')\n",
    "    pairs = []\n",
    "    for retrieved_doc_list, q_c_pair in zip(retrieved_docs_for_pairs, q_c_pairs):\n",
    "        question = q_c_pair[\"question\"]\n",
    "        for doc in retrieved_doc_list:\n",
    "            if int(chunk_size) > 1800 and int(chunk_size) < 3600:\n",
    "                # Split the text into two parts\n",
    "                part_size = len(doc.page_content) // 2\n",
    "                parts = [doc.page_content[:part_size], doc.page_content[part_size:]]\n",
    "                pairs.append([question, parts[0]])\n",
    "                pairs.append([question, parts[1]])\n",
    "            elif int(chunk_size) > 3600:\n",
    "                # Split the text into four parts\n",
    "                part_size = len(doc.page_content) // 4\n",
    "                parts = [\n",
    "                    doc.page_content[part_size * i:part_size * (i + 1)] for i in range(4)\n",
    "                ]\n",
    "                pairs.append([question, parts[0]])\n",
    "                pairs.append([question, parts[1]])\n",
    "                pairs.append([question, parts[2]])\n",
    "                pairs.append([question, parts[3]])\n",
    "            else: \n",
    "                pairs.append([question, doc.page_content])\n",
    "\n",
    "    # Calculate score for each doc\n",
    "    scores = reranker.compute_score(pairs, batch_size=16)\n",
    "    avg = sum(scores) / len(scores)\n",
    "\n",
    "    return avg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
