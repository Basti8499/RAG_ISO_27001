{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main notebook for evaluating the retrieval process of the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.a_setup_llms import create_embedding_model\n",
    "from ipynb.fs.defs.b_retrieve_data_and_prompt import retrieve_contexts\n",
    "from ipynb.fs.defs.b_retrieve_data_and_prompt import load_multiple_queries_list\n",
    "from ipynb.fs.defs.b_retrieve_data_and_prompt import load_hyde_docs\n",
    "from ipynb.fs.defs.c_evaluation_retrieval import generate_and_store_question_context_pairs\n",
    "from ipynb.fs.defs.c_evaluation_retrieval import load_question_context_pairs\n",
    "from ipynb.fs.defs.c_evaluation_retrieval import calculate_average_hit_rate\n",
    "from ipynb.fs.defs.c_evaluation_retrieval import calculate_average_mrr\n",
    "from ipynb.fs.defs.c_evaluation_retrieval import calculate_average_time\n",
    "from ipynb.fs.defs.c_evaluation_retrieval import calculate_average_similarity_score_BGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "import os\n",
    "\n",
    "new_client = chromadb.PersistentClient(path=os.environ.get(\"CHROMA_PATH\"))\n",
    "collections = new_client.list_collections()\n",
    "collections = [collection.name for collection in collections]\n",
    "print(collections)\n",
    "print(len(collections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out, if QC pairs need to be generated for the given collections.\n",
    "# generate_and_store_question_context_pairs(collections, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "\n",
    "def evaluate_retrieval(collection_names: List[str], retriever_methods: List[str], k_neighbours: List[int], result_name_path: str, rerank_k: int=50, dense_percent: float = 0.5):\n",
    "    \"\"\"\n",
    "    Evaluates the retrieval process of all given collections, retrieval methods and k_neighbours.\n",
    "    \"\"\"\n",
    "\n",
    "    # Path to store results\n",
    "    full_path = f\"./../../evaluationResults/retrievalEval/{result_name_path}\"\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Check if the df already exists, otherwise create a new one\n",
    "    try:\n",
    "        df = pd.read_csv(full_path)\n",
    "    except FileNotFoundError:\n",
    "        df = pd.DataFrame(columns=[\"index_name\", \"retriever_method\", \"number_retrieved_docs\", \"hit_rate\", \"mrr\", \"similarity\", \"duration\"])\n",
    "    \n",
    "    # Go through each given collection\n",
    "    for collection_name in collection_names:\n",
    "        new_client = chromadb.PersistentClient(path=os.environ.get(\"CHROMA_PATH\"))\n",
    "        vectordb = Chroma(client=new_client, collection_name=collection_name)\n",
    "\n",
    "        print(\"Starting with collection \" + collection_name)\n",
    "\n",
    "        if \"PC_\" in collection_name:\n",
    "            metadata = {\n",
    "                \"file_type\": vectordb._collection.metadata[\"file_type\"],\n",
    "                \"chunk_size\": vectordb._collection.metadata[\"chunk_size_parent\"],\n",
    "                \"chunk_overlap\": vectordb._collection.metadata[\"chunk_overlap_parent\"],\n",
    "                \"title_appended\": vectordb._collection.metadata[\"title_appended\"],\n",
    "                \"embedding_model_provider\": vectordb._collection.metadata[\"embedding_model_provider\"],\n",
    "                \"embedding_model_name\": vectordb._collection.metadata[\"embedding_model_name\"],\n",
    "            }\n",
    "        else:\n",
    "            metadata = {\n",
    "                \"file_type\": vectordb._collection.metadata[\"file_type\"],\n",
    "                \"chunk_size\": vectordb._collection.metadata[\"chunk_size\"],\n",
    "                \"chunk_overlap\": vectordb._collection.metadata[\"chunk_overlap\"],\n",
    "                \"title_appended\": vectordb._collection.metadata[\"title_appended\"],\n",
    "                \"embedding_model_provider\": vectordb._collection.metadata[\"embedding_model_provider\"],\n",
    "                \"embedding_model_name\": vectordb._collection.metadata[\"embedding_model_name\"],\n",
    "            }\n",
    "\n",
    "        # Create the embedding model based on metadata\n",
    "        embedding_model = create_embedding_model(metadata[\"embedding_model_provider\"], metadata[\"embedding_model_name\"])\n",
    "        vectordb = Chroma(client=new_client, collection_name=collection_name, embedding_function=embedding_model)\n",
    "        # Load the question, context pairs associated with the index's metadata\n",
    "        qc_object = load_question_context_pairs(metadata[\"chunk_size\"], metadata[\"chunk_overlap\"], metadata[\"file_type\"], \"False\")\n",
    "        qc_pairs = qc_object[\"question_context_pairs\"]\n",
    "\n",
    "        # Load HyDe Docs and Multi Query lists for chunk size and overlap combination if that is the retriever method\n",
    "        hyde_docs = []\n",
    "        multi_query_lists = []\n",
    "\n",
    "        if \"Multi_Query\" in retriever_methods or \"HyDE\" in retriever_methods or \"Hybrid_Multi_Query_Cohere\" in retriever_methods or \"Hybrid_Multi_Query\" in retriever_methods or \"Hybrid_HyDE\" in retriever_methods:\n",
    "            path_hyde = \"Hyde_\" + metadata[\"chunk_size\"] + \"_\" +  metadata[\"chunk_overlap\"] + \"_\"+ metadata[\"file_type\"] + \"_\" + metadata[\"title_appended\"]\n",
    "            path_multi = \"Multi_Query_\" + metadata[\"chunk_size\"] + \"_\" +  metadata[\"chunk_overlap\"] + \"_\"+ metadata[\"file_type\"] + \"_\" + metadata[\"title_appended\"]\n",
    "            hyde_docs = load_hyde_docs(True, path_hyde)\n",
    "            multi_query_lists = load_multiple_queries_list(True, path_multi)\n",
    "\n",
    "        # Iterate over all desired retrievers\n",
    "        for retriever_method in retriever_methods:\n",
    "            print(\"Starting with retriever method \" + retriever_method)\n",
    "            if \"Parent_Child\" in retriever_method and \"PC_\" not in collection_name:\n",
    "                continue\n",
    "            if \"Parent_Child\" not in retriever_method and \"PC_\" in collection_name:\n",
    "                continue\n",
    "                \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # For each retriever go through all questions\n",
    "            for k in k_neighbours:\n",
    "                docs_list = []\n",
    "                duration_list = []\n",
    "                \n",
    "                if \"HyDE\" in retriever_method:\n",
    "                    for qc_pair, hyde_doc in zip(qc_pairs, hyde_docs):\n",
    "                        retrieved_docs, duration = retrieve_contexts(vectordb=vectordb, retrieval_method=retriever_method,k=k, query=qc_pair[\"question\"], rerank_k= rerank_k, dense_percent=dense_percent, hyde_document=hyde_doc)\n",
    "                        docs_list.append(retrieved_docs)\n",
    "                        duration_list.append(duration)\n",
    "\n",
    "                elif \"Multi_Query\" in retriever_method:\n",
    "                    for qc_pair, multi_query_list in zip(qc_pairs, multi_query_lists):\n",
    "                        retrieved_docs, duration = retrieve_contexts(vectordb=vectordb, retrieval_method=retriever_method,k=k, query=qc_pair[\"question\"],rerank_k= rerank_k, dense_percent=dense_percent, multiple_queries=multi_query_list)\n",
    "                        docs_list.append(retrieved_docs)\n",
    "                        duration_list.append(duration)\n",
    "\n",
    "                else:\n",
    "                    for index, qc_pair in enumerate(qc_pairs):\n",
    "                        if index % 5 == 0:\n",
    "                            print(index)\n",
    "                        retrieved_docs, duration = retrieve_contexts(vectordb=vectordb, retrieval_method=retriever_method,k=k, query=qc_pair[\"question\"], rerank_k= rerank_k, dense_percent=dense_percent)\n",
    "                        docs_list.append(retrieved_docs)\n",
    "                        duration_list.append(duration)\n",
    "            \n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                avg_hit_rate = calculate_average_hit_rate(docs_list, qc_pairs)\n",
    "                avg_mmr = calculate_average_mrr(docs_list, qc_pairs)\n",
    "                avg_similarity_score = calculate_average_similarity_score_BGE(docs_list, qc_pairs, metadata[\"chunk_size\"])\n",
    "                avg_duration = calculate_average_time(duration_list)\n",
    "\n",
    "                new_row = pd.DataFrame({\"index_name\": collection_name,\"retriever_method\": retriever_method, \"number_retrieved_docs\": k, \"hit_rate\": avg_hit_rate, \"mrr\": avg_mmr, \"similarity\": avg_similarity_score, \"duration\": avg_duration}, index=[0])\n",
    "                df = pd.concat([new_row,df.loc[:]]).reset_index(drop=True)\n",
    "                df.to_csv(full_path, index=False)\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections_to_evaluate = [collections[0]]\n",
    "print(collections_to_evaluate)\n",
    "\n",
    "retriever_methods = [\"Hybrid_Rerank_Cohere\"]\n",
    "k_neighbours = [8]\n",
    "rerank_k = 50\n",
    "dense_percent = 0.5\n",
    "\n",
    "results_df = evaluate_retrieval(collections_to_evaluate, retriever_methods, k_neighbours, \"Retrieval_Eval.csv\", rerank_k, dense_percent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
