{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for calculating all metrics associated with the generation evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import List\n",
    "import torch\n",
    "import time\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ragas\n",
    "from datasets import Dataset\n",
    "from ragas.metrics import AnswerRelevancy, ContextRecall\n",
    "from ragas.llms import llm_factory\n",
    "\n",
    "# HuggingFace\n",
    "from sentence_transformers import CrossEncoder\n",
    "import evaluate\n",
    "\n",
    "# DeepEval\n",
    "from deepeval.scorer import Scorer\n",
    "\n",
    "# Embedding Model\n",
    "from ipynb.fs.defs.a_setup_llms import create_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_score_bulk(generated_texts: List[str], target_texts: List[str], score_type: str) -> Tuple[List[float], float]:\n",
    "    '''\n",
    "    Calculates the ROUGE score of the given score type between the generated LLM text and the golden answer.\n",
    "    '''\n",
    "    if score_type not in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "        raise Exception(\"Error, raised exception: Wrong score type for rouge score given.\")\n",
    "\n",
    "    scores = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for gen_text, target_text in zip(generated_texts, target_texts):\n",
    "        score = Scorer.rouge_score(target=target_text, prediction=gen_text, score_type=score_type)\n",
    "        scores.append(score)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_avg = (end_time - start_time) / len(generated_texts)\n",
    "       \n",
    "    return scores, time_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_response_length_bulk(generated_texts: List[str]) -> Tuple[int, float]:\n",
    "    '''\n",
    "    Calculates the response length (number of words) based on the NLTK word tokenizer.\n",
    "    '''    \n",
    "    start_time = time.time()\n",
    "    word_length = evaluate.load(\"word_length\", module_type=\"measurement\")\n",
    "    results = word_length.compute(data=generated_texts)\n",
    "    end_time = time.time()\n",
    "    time_avg = (end_time - start_time) / len(generated_texts)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return results[\"average_word_length\"], time_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hallucination_score_bulk(generated_texts: List[str], context_lists: List[List[str]]) -> Tuple[List[float], float]:\n",
    "    '''\n",
    "    Generates the hallucination score between the generated LLM text and the retrieved contexts.\n",
    "    Is based on the Vectara Hallucination Evaluation Model.\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "\n",
    "    pairs = []\n",
    "    # Create pairs of all generated_texts and the connected retrieved contexts\n",
    "    for gen, context_list in zip(generated_texts, context_lists):\n",
    "        for context in context_list:\n",
    "            pairs.append([context, gen])\n",
    "    \n",
    "    model = CrossEncoder('vectara/hallucination_evaluation_model')\n",
    "    scores = model.predict(pairs, batch_size=8)\n",
    "\n",
    "    avg_scores = []\n",
    "    index = 0\n",
    "\n",
    "    for context_list in context_lists:\n",
    "        # Calculate average score for the current context_list\n",
    "        scores_contexts = scores[index:index + len(context_list)]\n",
    "        avg_score = sum(scores_contexts) / len(scores_contexts)\n",
    "        avg_scores.append(avg_score)\n",
    "\n",
    "        # Move the index to the next set of scores\n",
    "        index += len(context_list)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_avg = (end_time - start_time) / len(generated_texts)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return avg_scores, time_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sentence_transformers import CrossEncoder\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "from FlagEmbedding import FlagReranker\n",
    "\n",
    "def calculate_answer_relevancy_bulk(generated_texts: List[str], questions: List[str]) -> Tuple[List[float], float]:\n",
    "    '''\n",
    "    Generates the answer relevancy score between the generated LLM text and the provided question.\n",
    "    Uses the BAAI/bge-reranker-base cross-encoder from HuggingFace.\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "\n",
    "    reranker = FlagReranker('BAAI/bge-reranker-base')\n",
    "\n",
    "    pairs = []\n",
    "    for question, gen_text in zip(questions, generated_texts):\n",
    "        pairs.append([question, gen_text])\n",
    "\n",
    "    scores = reranker.compute_score(pairs, batch_size=16)\n",
    "    end_time = time.time()\n",
    "\n",
    "    time_avg = (end_time - start_time) / len(generated_texts)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return scores, time_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_answer_similarity_bulk(generated_texts: List[str], target_texts: List[str]) -> Tuple[List[float], float]:\n",
    "    '''\n",
    "    Generates the answer relevancy between the generated LLM texts and target texts by using the fine-tuned ISO-bge model to compute the cosine similarity.\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "\n",
    "    embedding_model = create_embedding_model(\"Fine-tuned\", \"finetuned-ISO-27001_1024\")\n",
    "    embeddings_gen_texts = embedding_model.embed_documents(generated_texts)\n",
    "    embeddings_target_texts = embedding_model.embed_documents(target_texts)\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    cosine_sim_matrix = cosine_similarity(embeddings_gen_texts, embeddings_target_texts)\n",
    "\n",
    "    print(len(cosine_sim_matrix))\n",
    "    # Extract the diagonal to get the similarity scores for corresponding pairs\n",
    "    cosine_sim_scores = np.diag(cosine_sim_matrix)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_avg = (end_time - start_time) / len(generated_texts)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return cosine_sim_scores, time_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_context_recall_bulk(contexts: List[List[str]], target_texts: List[str], questions: List[str]) -> Tuple[List[float], float]:\n",
    "    '''\n",
    "    This method computes the context recall which measures the extent to which the retrieved contexts aligns with the golden answer.\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "\n",
    "    llm = llm_factory(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "    context_recall = ContextRecall(batch_size=10, llm=llm)\n",
    "    dataset = Dataset.from_dict({\"contexts\": contexts, \"ground_truths\": target_texts, \"question\": questions})\n",
    "    results = context_recall.score(dataset)\n",
    "    scores = results[\"context_recall\"]\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_avg = (end_time - start_time) / len(questions)\n",
    "\n",
    "    return scores, time_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_answer_relevancy_RAGAS_bulk(questions: List[str], answers: List[str]) -> Tuple[List[float], float]:\n",
    "    \"\"\"\n",
    "    Computes the answer relevancy score based using RAGAS. Uses an LLM to generate multiple fitting questions based on the generated answer. Then proceeds to compute the cosine similarity between the generated question and the given question. Has the underlying idea that if the generated answer accurately answers the question, the LLM should be able to generate questions that align with the original one. Values are ranging from 0 to 1, with 1 being the highest relevance value\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    embedding_model = create_embedding_model(\"Fine-tuned\", \"finetuned-ISO-27001_1024\")\n",
    "    llm = llm_factory(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "    answer_relevancy = AnswerRelevancy(batch_size=10, embeddings=embedding_model, llm=llm)\n",
    "    dataset = Dataset.from_dict({\"question\": questions, \"answer\": answers})\n",
    "    results = answer_relevancy.score(dataset)\n",
    "    scores = results[\"answer_relevancy\"]\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_avg = (end_time - start_time) / len(questions)\n",
    "\n",
    "    return scores, time_avg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
