{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for setting up LLMs and embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Replicate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from transformers import pipeline\n",
    "from mistralai.client import MistralClient\n",
    "import os\n",
    "\n",
    "\n",
    "def setup_llm(model_provider, model_name, temperature):\n",
    "    \"\"\"\n",
    "    Method for returning the specified LLM. Possible combinations are (modelProvider: modelName):\n",
    "    - Replicate: lLama2-7b-chat | lLama2-13b-chat | lLama2-70b-chat [https://replicate.com/blog/run-llama-2-with-an-api]\n",
    "    - OpenAI: gpt-3.5-turbo | gpt-3.5-turbo-16k | gpt-4 [https://platform.openai.com/docs/models]\n",
    "    - HuggingFace: flan-t5-large [https://huggingface.co/docs/transformers/model_doc/flan-t5]\n",
    "    \"\"\"\n",
    "    if model_provider == \"Replicate\":\n",
    "        if temperature == 0:\n",
    "            temperature += 0.01\n",
    "        if model_name == \"lLama2-7b-chat\":\n",
    "            llm = Replicate(\n",
    "                model=\"meta/llama-2-7b-chat:13c3cdee13ee059ab779f0291d29054dab00a47dad8261375654de5540165fb0\",\n",
    "                model_kwargs={\"temperature\": temperature, \"max_new_tokens\": 300, \"max_length\": 300},\n",
    "            )\n",
    "            max_context_size = 4096\n",
    "            return llm, max_context_size\n",
    "        elif model_name == \"lLama2-13b-chat\":\n",
    "            llm = Replicate(\n",
    "                model=\"meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\",\n",
    "                model_kwargs={\"temperature\": temperature, \"max_new_tokens\": 300, \"max_length\": 300},\n",
    "            )\n",
    "            max_context_size = 4096\n",
    "            return llm, max_context_size\n",
    "        elif model_name == \"lLama2-70b-chat\":\n",
    "            llm = Replicate(\n",
    "                model=\"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "                model_kwargs={\"temperature\": temperature, \"max_new_tokens\": 300, \"max_length\": 300},\n",
    "            )\n",
    "            max_context_size = 4096\n",
    "            return llm, max_context_size\n",
    "\n",
    "    elif model_provider == \"OpenAI\":\n",
    "        if model_name == \"gpt-3.5-turbo\":\n",
    "            llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0613\", temperature=temperature, max_tokens=300)\n",
    "            max_context_size = 4096\n",
    "            return llm, max_context_size\n",
    "        elif model_name == \"gpt-3.5-turbo-16k\":\n",
    "            llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=temperature, max_tokens=300)\n",
    "            max_context_size = 16385\n",
    "            return llm, max_context_size\n",
    "        elif model_name == \"gpt-4\":\n",
    "            llm = ChatOpenAI(model_name=\"gpt-4-0613\", temperature=temperature, max_tokens=300)\n",
    "            max_context_size = 8192\n",
    "            return llm, max_context_size\n",
    "\n",
    "    elif model_provider == \"HuggingFace\":\n",
    "        if model_name == \"flan-t5-large\":\n",
    "            llm = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
    "            max_context_size = 2048\n",
    "            return llm, max_context_size\n",
    "\n",
    "    elif model_provider == \"Mistral\":\n",
    "        if model_name == \"mixtral-8x7B-v0.1\":\n",
    "            api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "            llm = MistralClient(api_key=api_key)\n",
    "            return llm, 16000\n",
    "    else:\n",
    "        raise Exception(\"Error, raised exception: Wrong modelProvider or modelName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_LLMs():\n",
    "    \"\"\" \n",
    "    Initiliazes all available LLMs. Can be used before evaluating to reduce errors.\n",
    "    \"\"\"\n",
    "    llms_dict = [\n",
    "        {\"model_provider\": \"Replicate\", \"model_name\": \"lLama2-7b-chat\", \"temperature\": 0, \"max_context_size\": 4096},\n",
    "        {\"model_provider\": \"Replicate\", \"model_name\": \"lLama2-13b-chat\", \"temperature\": 0, \"max_context_size\": 16385},\n",
    "        {\"model_provider\": \"Replicate\", \"model_name\": \"lLama2-70b-chat\", \"temperature\": 0, \"max_context_size\": 4096},\n",
    "        {\"model_provider\": \"OpenAI\", \"model_name\": \"gpt-3.5-turbo\", \"temperature\": 0, \"max_context_size\": 4096},\n",
    "        {\"model_provider\": \"OpenAI\", \"model_name\": \"gpt-3.5-turbo-16k\", \"temperature\": 0, \"max_context_size\": 16385},\n",
    "        {\"model_provider\": \"OpenAI\", \"model_name\": \"gpt-4\", \"temperature\": 0, \"max_context_size\": 8192},\n",
    "        {\"model_provider\": \"HuggingFace\", \"model_name\": \"flan-t5-large\", \"temperature\": 0, \"max_context_size\": 2048},\n",
    "        {\"model_provider\": \"Mistral\", \"model_name\": \"mixtral-8x7B-v0.1\", \"temperature\": 0, \"max_context_size\": 16000}]\n",
    "\n",
    "    llms = []\n",
    "    sizes = []\n",
    "    for i in llms_dict:\n",
    "        print(i[\"model_name\"])\n",
    "        llm, size = setup_llm(i[\"model_provider\"], i[\"model_name\"], 0)\n",
    "        llms.append(llm)\n",
    "        sizes.append(size)\n",
    "\n",
    "    print(llms)\n",
    "    print(\"\\n\")\n",
    "    print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import CohereEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import VoyageEmbeddings\n",
    "\n",
    "def create_embedding_model(model_provider: str, model_name: str):\n",
    "    \"\"\"\n",
    "    Creates the embedding model and returns it. Possible combinations are (modelProvider: modelName (embedding size, max input length))\n",
    "    - Cohere: v2 (4096, 512) | v3 (1024, 512) [https://docs.cohere.com/reference/embed]\n",
    "    - OpenAI: text-embedding-ada-002 (1536, 8191) [https://platform.openai.com/docs/guides/embeddings]\n",
    "    - Voyage: voyage-lite-01 (1024, 4096) [https://docs.voyageai.com/embeddings/]\n",
    "    - HuggingFace: \n",
    "        - all-mpnet-base-v2 (768, 384) [https://huggingface.co/sentence-transformers/all-mpnet-base-v2]\n",
    "        - all-MiniLM-L6-v2 (384, 256) [https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2]\n",
    "        - bge-large-en-v1.5 (1024, 512) [https://huggingface.co/BAAI/bge-large-en-v1.5]\n",
    "        - SecRoBERTa (824, 512) [https://huggingface.co/jackaduma/SecRoBERTa]\n",
    "    - Fine-tuned:\n",
    "        - finetuned-ISO-27001_1024 (1024, 512) [https://huggingface.co/Basti8499/bge-large-en-v1.5-ISO-27001]\n",
    "    \"\"\"\n",
    "\n",
    "    if model_provider == \"Cohere\":\n",
    "        if model_name == \"v2\":\n",
    "            embeddings = CohereEmbeddings(model=\"embed-english-v2.0\")\n",
    "            print(\"Cohere v2 embedding: Vector embedding size - 4096, input length: 512\")\n",
    "            return embeddings\n",
    "        if model_name == \"v3\":\n",
    "            embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\n",
    "            print(\"Cohere v3 embedding: Vector embedding size - 1024, input length: 512\")\n",
    "            return embeddings\n",
    "\n",
    "    elif model_provider == \"HuggingFace\":\n",
    "        if model_name == \"all-mpnet-base-v2\":\n",
    "            embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "            print(\"HuggingFace all-mpnet-base-v2 embedding - Vector embedding size: 768, input length: 384\")\n",
    "            return embeddings\n",
    "        if model_name == \"all-MiniLM-L6-v2\":\n",
    "            embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "            print(\"HuggingFace all-MiniLM-L6-v2 embedding - Vector embedding size: 384, input length: 256\")\n",
    "            return embeddings\n",
    "        if model_name == \"bge-large-en-v1.5\":\n",
    "            embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "            print(\"HuggingFace BAAI/bge-large-en-v1.5 embedding - Vector embedding size: 1024, input length: 512\")        \n",
    "            return embeddings\n",
    "        if model_name == \"Contriever\":\n",
    "            embeddings = HuggingFaceEmbeddings(model_name = \"facebook/contriever-msmarco\")\n",
    "            print(\"HuggingFace facebook/contriever-msmarco embedding - Vector embedding size: 768, input length: 512\")   \n",
    "            return embeddings     \n",
    "        if model_name == \"SecRoBERTa\":\n",
    "            embeddings = HuggingFaceEmbeddings(model_name=\"jackaduma/SecRoBERTa\")\n",
    "            print(\"HuggingFace jackaduma/SecRoBERTa embedding - Vector embedding size: 768, input length: 512\")        \n",
    "            return embeddings\n",
    "        \n",
    "    elif model_provider == \"Voyage\":\n",
    "        if model_name == \"voyage-2\":\n",
    "            embeddings = VoyageEmbeddings(model=\"voyage-2\", show_progress_bar=True, batch_size=200)\n",
    "            print(\"Voyage embedding - Vector embedding size: 1024, input length: 4096\")\n",
    "            return embeddings \n",
    "        \n",
    "    elif model_provider == \"OpenAI\":\n",
    "        if model_name == \"text-embedding-ada\":\n",
    "            embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "            print(\"OpenAI embedding - Vector embedding size: 1536, input length: 8191\")\n",
    "            print(\"Tokenizer used: cl100k_base\")\n",
    "            return embeddings\n",
    "        \n",
    "    elif model_provider == \"Fine-tuned\":\n",
    "        if model_name == \"finetuned-ISO-27001_1024\":\n",
    "            embeddings = HuggingFaceEmbeddings(model_name=\"Basti8499/bge-large-en-v1.5-ISO-27001\")\n",
    "            print(\"Fine-tuned bge-large-en-v1.5 with ISO 27001 - Vector embedding size: 1024, input length: 512\")   \n",
    "            return embeddings                 \n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"Error, raised exception: Wrong modelProvider or modelName provided.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
