{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for the whole retrieval and generation process. Include checking the context size, creating content for Multi-Query and HyDE, retrieving the relevant documents and prompting the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from transformers import T5Tokenizer\n",
    "from transformers import LlamaTokenizerFast\n",
    "from langchain.docstore.document import Document\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def is_context_size_valid(model_provider: str, model_name: str, contexts: List[Document], query: str, max_context_size: int) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if the context size of the query + contexts fits into the context size of the LLM\n",
    "    \"\"\"\n",
    "    \n",
    "    # Same prompts as in the query llm method\n",
    "    system_prompt = \"You are an expert in information security, especially for ISO 27001 certifications. Answer the following question as truthfully as possible, using the provided context. If the answer is not contained within the context or the question is not related to the topic of information security or ISO 27001, respond with 'I don't know\"\n",
    "\n",
    "    concatenated_contexts = \"\"\n",
    "    for index, document in enumerate(contexts, start=1):\n",
    "        original_text = document.metadata.get(\"original_text\", \"\")\n",
    "        concatenated_contexts += f\"{index}. {original_text}\\n\\n\"\n",
    "\n",
    "    if not query.endswith(\"?\"):\n",
    "        query = query + \"?\"\n",
    "    context_question_formatted = f\"Context: {concatenated_contexts} \\n Question: {query}\"\n",
    "    full_prompt = system_prompt + \"\\n\" + context_question_formatted\n",
    "    # If the llm is from OpenAI, use the cl100k_base tokenizer\n",
    "    if model_provider == \"OpenAI\":\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        token_length = len(tokenizer.encode(full_prompt))\n",
    "\n",
    "    # If the llm is from Replicate (so in use-case only Llama models), use the Llama Tokenizer from HF\n",
    "    elif model_provider == \"Replicate\":\n",
    "        tokenizer = LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "        token_length = len(tokenizer.encode(full_prompt))\n",
    "\n",
    "    # If the llm is from HF, use T5 tokenizer\n",
    "    elif model_provider == \"HuggingFace\":\n",
    "        if model_name == \"flan-t5-large\":\n",
    "            tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "            tokenizer.model_max_length = 2048\n",
    "            token_length = len(tokenizer(full_prompt).input_ids)\n",
    "\n",
    "    elif model_provider == \"Mistral\":\n",
    "        if model_name == \"mixtral-8x7B-v0.1\":\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "            token_length = len(tokenizer(full_prompt).input_ids)\n",
    "    else:\n",
    "        raise Exception(\"Error, raised exception: Wrong model_provider or model_name provided.\")\n",
    "\n",
    "    if token_length <= max_context_size:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_tokenizers():\n",
    "    \"\"\" \n",
    "    Initiliazes all tokenizers. Can be used before evaluating to already load them into the cache and reduce errors.\n",
    "    \"\"\"\n",
    "    models = [\n",
    "        {\"model_provider\": \"OpenAI\", \"model_name\": \"..\", \"max_context_size\": 3000},\n",
    "        {\"model_provider\": \"Replicate\", \"model_name\": \"..\", \"max_context_size\": 3000},\n",
    "        {\"model_provider\": \"HuggingFace\", \"model_name\": \"flan-t5-large\", \"max_context_size\": 300},\n",
    "        {\"model_provider\": \"Mistral\", \"model_name\": \"mixtral-8x7B-v0.1\", \"max_context_size\": 1}\n",
    "    ]\n",
    "    d1 = Document(page_content=\"Test\", metadata={\"original_text\": \"test\"})\n",
    "    d2 = Document(page_content=\"Test\", metadata={\"original_text\": \"test\"})\n",
    "    contexts = [d1, d2]\n",
    "    query = \"test\"\n",
    "\n",
    "    for model in models:\n",
    "        print(c[\"model_provider\"])\n",
    "        print(is_context_size_valid(model[\"model_provider\"], model[\"model_name\"], contexts, query, model[\"max_context_size\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "from typing import List\n",
    "\n",
    "# Helper methods for storing and loading already generated documents\n",
    "def store_documents(documents, file_path: str) -> None:\n",
    "    with open(file_path, \"w\") as jsonl_file:\n",
    "        for doc in documents:\n",
    "            jsonl_file.write(doc.json() + \"\\n\")\n",
    "\n",
    "\n",
    "def store_queries(multiple_queries, file_path: str) -> None:\n",
    "    json_string = json.dumps(multiple_queries)\n",
    "    with open(file_path, \"w\") as jsonl_file:\n",
    "        jsonl_file.write(json_string)\n",
    "\n",
    "\n",
    "def load_documents(file_path: str) -> List[Document]:\n",
    "    documents = []\n",
    "    with open(file_path, \"r\") as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            data = json.loads(line)\n",
    "            obj = Document(**data)\n",
    "            documents.append(obj)\n",
    "    return documents\n",
    "\n",
    "\n",
    "def load_queries(file_path: str) -> List[List[str]]:\n",
    "    with open(file_path, \"r\") as jsonl_file:\n",
    "        queries = json.load(jsonl_file)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from typing import Any\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "\n",
    "def generate_multiple_queries(query: str, number_of_queries: int):\n",
    "    '''\n",
    "    Generates multiple (number_of_queries) queries based on the given query. Returns the original query with index 0 and all other generated ones. Is used for the Multi-Query retrieval strategy. Uses the ChatOpenAI API and a specific query.\n",
    "    '''\n",
    "\n",
    "    llm = OpenAI()\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(\"\"\"You are a helpful assistant that generates multiple search queries based on a single input query\"\"\")\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(\"\"\"Generate multiple search queries related to: {query}. Output exactly {number_of_queries} queries! For each query use a new line. Do not use any form of enumeration.\"\"\")\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "    prompts = chat_prompt.format_prompt(query=query, number_of_queries=number_of_queries).to_messages()\n",
    "    text = prompts[0].content + \"\\n\" + prompts[1].content\n",
    "\n",
    "    generated_queries = [query]\n",
    "\n",
    "    generated_queries_answer = llm(text)\n",
    "    generated_queries_answer_list = generated_queries_answer.strip().split(\"\\n\")\n",
    "\n",
    "    generated_queries.extend(generated_queries_answer_list)\n",
    "\n",
    "    return generated_queries\n",
    "\n",
    "\n",
    "def generate_and_store_multiple_queries_list(query_list: List[str], is_retrieval_eval: bool, number_of_queries: int, path_to_save: str) -> List[List[str]]:\n",
    "    '''\n",
    "    Calls the generate_multiple_queries for a list of queries and stores it under path_to_save. The path_to_save should include all the relevant metadata for the specific index, in order to reload the multiple queries.\n",
    "\n",
    "    Path should be in this format: \"Multi_Query_\" + metadata[\"chunk_size\"] + \"_\" +  metadata[\"chunk_overlap\"] + \"_\"+ metadata[\"file_type\"] + \"_\" + metadata[\"title_appended\"]\n",
    "    '''\n",
    "    multiple_query_lists = []\n",
    "\n",
    "    for index, query in enumerate(query_list):\n",
    "        print(index)\n",
    "        multiple_queries = generate_multiple_queries(query, number_of_queries)\n",
    "        multiple_query_lists.append(multiple_queries)\n",
    "\n",
    "    if is_retrieval_eval:\n",
    "        store_queries(multiple_query_lists,\n",
    "                        f\"./../../retrievalInput/Queries/Retrieval_Eval/{path_to_save}.json\")\n",
    "    else:\n",
    "        store_queries(multiple_query_lists,\n",
    "                        f\"./../../retrievalInput/Queries/Generation_Eval/{path_to_save}.json\")\n",
    "\n",
    "    return multiple_query_lists\n",
    "\n",
    "\n",
    "def load_multiple_queries_list(is_retrieval_eval: bool, path_to_load: str) -> List[List[str]]:\n",
    "    if is_retrieval_eval:\n",
    "        multiple_queries = load_queries(\n",
    "            f\"./../../retrievalInput/Queries/Retrieval_Eval/{path_to_load}.json\")\n",
    "    else:\n",
    "        multiple_queries = load_queries(\n",
    "            f\"./../../retrievalInput/Queries/Generation_Eval/{path_to_load}.json\")\n",
    "        \n",
    "    return multiple_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from typing import Any\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "def generate_hyde_doc(query: str) -> Document:\n",
    "    \"\"\"\n",
    "    Generates a document based on the given query. Is used for the HyDE retriever.\n",
    "    Uses the ChatOpenAI API and a specific query.\n",
    "    \"\"\"\n",
    "\n",
    "    llm = OpenAI()\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
    "        \"\"\"You are an expert in information security, especially for ISO 27001 certifications. Please write a short passage (3-4 sentences) to answer the question.\"\"\"\n",
    "    )\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(\"\"\"Question: {question}\"\"\")\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "    prompts = chat_prompt.format_prompt(question=query).to_messages()\n",
    "    text = prompts[0].content + \"\\n\" + prompts[1].content\n",
    "\n",
    "    answer = llm(text)\n",
    "\n",
    "    # Create a document with the LLM answer and the original query.\n",
    "    document = Document(page_content=answer, metadata={\"original_prompt\": query})\n",
    "    \n",
    "    return document\n",
    "\n",
    "def generate_and_store_multiple_hyde_docs(query_list: List[str], is_retrieval_eval: bool, path_to_save: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Calls the generate_multiple_hyde_docs for a list of queries and stores it.\n",
    "    Path should be in this format: \"Hyde_\" + metadata[\"chunk_size\"] + \"_\" +  metadata[\"chunk_overlap\"] + \"_\"+ metadata[\"file_type\"] + \"_\" + metadata[\"title_appended\"]\n",
    "    \"\"\"\n",
    "    hyde_docs = []\n",
    "\n",
    "    for index, query in enumerate(query_list):\n",
    "        print(index)\n",
    "        hyde_doc = generate_hyde_doc(query)\n",
    "        hyde_docs.append(hyde_doc)\n",
    "\n",
    "    if is_retrieval_eval:\n",
    "        store_documents(hyde_docs, f\"./../../retrievalInput/HyDE_Documents/Retrieval_Eval/{path_to_save}\")\n",
    "    else:\n",
    "        store_documents(hyde_docs, f\"./../../retrievalInput/HyDE_Documents/Generation_Eval/{path_to_save}\")\n",
    "\n",
    "    return hyde_docs\n",
    "\n",
    "\n",
    "def load_hyde_docs(is_retrieval_eval: bool, path_to_load: str) -> List[List[str]]:\n",
    "    if is_retrieval_eval:\n",
    "        hyde_docs = load_documents(f\"./../../retrievalInput/HyDE_Documents/Retrieval_Eval/{path_to_load}.json\")\n",
    "    else:\n",
    "        hyde_docs = load_documents(f\"./../../retrievalInput/HyDE_Documents/Generation_Eval/{path_to_load}.json\")\n",
    "\n",
    "    return hyde_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean pooling helper method for Contriever\n",
    "def mean_pooling(token_embeddings, mask):\n",
    "    token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "    sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python file for retrieving relevant documents and generating answer with LLM\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever, TFIDFRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.retrievers import MultiVectorRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain.storage.file_system import LocalFileStore\n",
    "from langchain.storage._lc_store import create_kv_docstore\n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from FlagEmbedding import FlagReranker\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def retrieve_contexts(vectordb, retrieval_method: str, k: int, query: str, rerank_k: int=50, dense_percent: float = 0.5, hyde_document: Document=None, multiple_queries: List[str]=[]):\n",
    "    \"\"\"\n",
    "    Method for all retrieval strategies. Returns k documents.\n",
    "\n",
    "    retrieval methods: Dense, BM25, TF-IDF, Rerank_Hybrid_BM25_Dense, Rerank_Hybrid_TF-IDF_Dense, Multi_Query, Hybrid_Multi_Query, Hybrid_Multi_Query_Cohere, Rerank_Cohere, Hybrid_Rerank_Cohere, Hybrid_Rerank_Cohere_Compression, Rerank_Contriever, Hybrid_Rerank_Contriever, Rerank_Cross_Encoder_Ms_Marco, Hybrid_Rerank_Cross_Encoder_Ms_Marco, Rerank_Cross_Encoder_BGE, Hybrid_Rerank_Cross_Encoder_BGE, HyDE, Hybrid_HyDE, MMR, Hybrid_MMR, Hybrid_MMR_Cohere, Parent_Child, Hybrid_Parent_Child, Hybrid_Parent_Child_Cohere, Hybrid_Parent_Child_MMR_Cohere\n",
    "    rerank_k: Determines how many documents should be retrieved from the vector database, if a re-ranker is used.\n",
    "    dense_percent: Determines how many percent of the documents should be retrieved from the dense index and the sparse index, if a hybrid retrieval strategy is used.\n",
    "    hyde_document: The provided HyDE doc, if the HyDE strategy is used. If it is empty and HyDE is used, generates a new one.\n",
    "    multiple_queries: The provided list of queries, if the Multi Query strategy is used. If it is empty three new queries based on the prompt are generated.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    query = query.lower()\n",
    "\n",
    "    documents = []\n",
    "    if \"Hybrid\" in retrieval_method or retrieval_method == \"BM25\" or retrieval_method == \"TF-IDF\" or retrieval_method == \"Rerank_Hybrid_BM25_Dense\" or retrieval_method == \"Rerank_Hybrid_TF-IDF_Dense\":\n",
    "\n",
    "        if \"Parent_Child\" in retrieval_method:\n",
    "            chunk_size_parent = vectordb._collection.metadata[\"chunk_size_parent\"]\n",
    "            chunk_overlap_parent = vectordb._collection.metadata[\"chunk_overlap_parent\"]\n",
    "            chunk_size_child = vectordb._collection.metadata[\"chunk_size_child\"]\n",
    "            chunk_overlap_child = vectordb._collection.metadata[\"chunk_overlap_child\"]\n",
    "            title_appended = vectordb._collection.metadata[\"title_appended\"]\n",
    "            file_type = vectordb._collection.metadata[\"file_type\"]\n",
    "            \n",
    "            document_file_name = str(chunk_size_parent) + \"_\" + str(chunk_overlap_parent) + \"_PC_\" + str (chunk_size_child) + \"_\" + str(chunk_overlap_child) + \"_\" + file_type + \"_\" + str(title_appended)\n",
    "            documents = load_documents(f\"./../../retrievalInput/Documents_For_Sparse/{document_file_name}\")\n",
    "        else:\n",
    "            chunk_size = vectordb._collection.metadata[\"chunk_size\"]\n",
    "            chunk_overlap = vectordb._collection.metadata[\"chunk_overlap\"]\n",
    "            title_appended = vectordb._collection.metadata[\"title_appended\"]\n",
    "            file_type = vectordb._collection.metadata[\"file_type\"]\n",
    "\n",
    "            document_file_name = str(chunk_size) + \"_\" + str(chunk_overlap) + \"_\" + file_type + \"_\" + \"False\"\n",
    "            documents = load_documents(f\"./../../retrievalInput/Documents_For_Sparse/{document_file_name}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if retrieval_method == \"Dense\":\n",
    "        result_documents = vectordb.similarity_search(\n",
    "            query, k=k)\n",
    "    \n",
    "    elif retrieval_method == \"BM25\" or retrieval_method == \"TF-IDF\" or retrieval_method == \"Rerank_Hybrid_BM25_Dense\" or retrieval_method == \"Rerank_Hybrid_TF-IDF_Dense\":\n",
    "        # Populate the information for getting the documents for the sparse retrieval with the same metadata as used with the index\n",
    "\n",
    "        if retrieval_method == \"BM25\":\n",
    "            bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "            bm25_retriever.k = k\n",
    "            result_documents = bm25_retriever.get_relevant_documents(query)\n",
    "            \n",
    "        if retrieval_method == \"TF-IDF\":\n",
    "            tf_idf_retriever = TFIDFRetriever.from_documents(documents)\n",
    "            tf_idf_retriever.k = k\n",
    "            result_documents = tf_idf_retriever.get_relevant_documents(query)\n",
    "\n",
    "        # Create an EnsembleRetriever with the dense and sparse (BM25) retrievers, then get the relevant documents. Automaticall reranks them based on Reciprocal Rank Fusion Algorithm\n",
    "        if retrieval_method == \"Rerank_Hybrid_BM25_Dense\":\n",
    "            bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "            bm25_retriever.k = k\n",
    "            vectordb_retriever = vectordb.as_retriever(\n",
    "                search_kwargs={\"k\": k}, search_type=\"similarity\")\n",
    "            ensemble_retriever = EnsembleRetriever(\n",
    "                retrievers=[bm25_retriever, vectordb_retriever], weights=[0.5, 0.5])\n",
    "            result_documents = ensemble_retriever.get_relevant_documents(\n",
    "                query)\n",
    "            result_documents = result_documents[:k]\n",
    "\n",
    "        # Create an EnsembleRetriever with the dense and sparse (TF-IDF) retrievers, then get the relevant documents. Automaticall reranks them based on Reciprocal Rank Fusion Algorithm\n",
    "        elif retrieval_method == \"Rerank_Hybrid_TF-IDF_Dense\":\n",
    "            tf_idf_retriever = TFIDFRetriever.from_documents(documents)\n",
    "            tf_idf_retriever.k = k\n",
    "            vectordb_retriever = vectordb.as_retriever(\n",
    "                search_kwargs={\"k\": k}, search_type=\"similarity\")\n",
    "            ensemble_retriever = EnsembleRetriever(\n",
    "                retrievers=[tf_idf_retriever, vectordb_retriever], weights=[0.5, 0.5])\n",
    "            result_documents = ensemble_retriever.get_relevant_documents(\n",
    "                query)\n",
    "            result_documents = result_documents[:k]\n",
    "\n",
    "    # Use the ensemble retriever to rerank as it uses the Reciprocal Rank Fusion Algorithm. Then retrieve the first k documents\n",
    "    elif retrieval_method == \"Multi_Query\":\n",
    "        retrievers = []\n",
    "        relevant_document_list = []\n",
    "\n",
    "        if len(multiple_queries) == 0:\n",
    "            multiple_queries = generate_multiple_queries(query, 3)\n",
    "\n",
    "        # Generate an own retriever (necessary for using the ensemble) for each query and find relevant documents\n",
    "        for query in multiple_queries:\n",
    "            vectordb_retriever = vectordb.as_retriever(search_kwargs={\"k\": k}, search_type=\"similarity\")\n",
    "            retrievers.append(vectordb_retriever)\n",
    "            documents = vectordb_retriever.get_relevant_documents(query)\n",
    "            relevant_document_list.append(documents)\n",
    "\n",
    "        # Create an ensemble retriever\n",
    "        ensemble_retriever = EnsembleRetriever(retrievers=retrievers)\n",
    "        # Rerank the already retrieved documents\n",
    "        result_documents = ensemble_retriever.weighted_reciprocal_rank(relevant_document_list)\n",
    "        # Give back the top k documents\n",
    "        result_documents = result_documents[:k]\n",
    "\n",
    "    elif retrieval_method == \"Hybrid_Multi_Query\":\n",
    "\n",
    "        retrievers = []\n",
    "        relevant_document_list = []\n",
    "\n",
    "        if len(multiple_queries) == 0:\n",
    "            multiple_queries = generate_multiple_queries(query, 3)\n",
    "\n",
    "        # Generate an own retriever (necessary for using the ensemble) for each query and find relevant documents\n",
    "        for query in multiple_queries:\n",
    "            vectordb_retriever = vectordb.as_retriever(search_kwargs={\"k\": k}, search_type=\"similarity\")\n",
    "            retrievers.append(vectordb_retriever)\n",
    "            documents = vectordb_retriever.get_relevant_documents(query)\n",
    "            relevant_document_list.append(documents)\n",
    "\n",
    "        for query in multiple_queries:\n",
    "            bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "            bm25_retriever.k = k\n",
    "            retrievers.append(bm25_retriever)\n",
    "            documents = bm25_retriever.get_relevant_documents(query)\n",
    "            relevant_document_list.append(documents)\n",
    "\n",
    "        # Create an ensemble retriever\n",
    "        ensemble_retriever = EnsembleRetriever(retrievers=retrievers)\n",
    "        # Rerank the already retrieved documents\n",
    "        result_documents = ensemble_retriever.weighted_reciprocal_rank(relevant_document_list)\n",
    "        # Give back the top k documents\n",
    "        result_documents = result_documents[:k]\n",
    "\n",
    "    elif retrieval_method == \"Hybrid_Multi_Query_Cohere\":\n",
    "\n",
    "        retrievers = []\n",
    "        relevant_document_list = []\n",
    "\n",
    "        if len(multiple_queries) == 0:\n",
    "            multiple_queries = generate_multiple_queries(query, 3)\n",
    "\n",
    "        # Generate an own retriever (necessary for using the ensemble) for each query and find relevant documents\n",
    "        for query in multiple_queries:\n",
    "            vectordb_retriever = vectordb.as_retriever(search_kwargs={\"k\": k}, search_type=\"similarity\")\n",
    "            retrievers.append(vectordb_retriever)\n",
    "            documents = vectordb_retriever.get_relevant_documents(query)\n",
    "            relevant_document_list.extend(documents)\n",
    "\n",
    "        for query in multiple_queries:\n",
    "            bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "            bm25_retriever.k = k\n",
    "            retrievers.append(bm25_retriever)\n",
    "            documents = bm25_retriever.get_relevant_documents(query)\n",
    "            relevant_document_list.extend(documents)\n",
    "\n",
    "        unique_documents_dict = {}\n",
    "        for doc in relevant_document_list:\n",
    "            if doc.page_content not in unique_documents_dict:\n",
    "                unique_documents_dict[doc.page_content] = doc\n",
    "\n",
    "        # Extracting the unique documents from the dictionary\n",
    "        result_documents_unique = list(unique_documents_dict.values())\n",
    "\n",
    "        compressor = CohereRerank(top_n=k, user_agent=\"langchain\")\n",
    "        result_documents = compressor.compress_documents(documents=result_documents_unique, query=query)\n",
    "        \n",
    "    # Uses dense retrieval and Cohere reranking (always retrieve 50 documents and then return the top k after reranking)\n",
    "    elif retrieval_method == \"Rerank_Cohere\":\n",
    "        vectordb_retriever = vectordb.as_retriever(search_kwargs={\"k\": 50}, search_type=\"similarity\")\n",
    "        compressor = CohereRerank(top_n=k, user_agent=\"langchain\")\n",
    "        retriever_rerank = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=vectordb_retriever)\n",
    "        result_documents = retriever_rerank.get_relevant_documents(query=query)\n",
    "\n",
    "    elif retrieval_method == \"Hybrid_Rerank_Cohere\":\n",
    "        dense_k = int(rerank_k * dense_percent)\n",
    "        sparse_k = rerank_k - dense_k\n",
    "\n",
    "        bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "        bm25_retriever.k = sparse_k\n",
    "        result_documents_BM25 = bm25_retriever.get_relevant_documents(query)\n",
    "\n",
    "        result_documents_Dense = vectordb.similarity_search(query, k=dense_k)\n",
    "\n",
    "        result_documents_all = []\n",
    "        result_documents_all.extend(result_documents_BM25)\n",
    "        result_documents_all.extend(result_documents_Dense)\n",
    "\n",
    "        unique_documents_dict = {}\n",
    "\n",
    "        for doc in result_documents_all:\n",
    "            if doc.page_content not in unique_documents_dict:\n",
    "                unique_documents_dict[doc.page_content] = doc\n",
    "\n",
    "        # Extracting the unique documents from the dictionary\n",
    "        result_documents_unique = list(unique_documents_dict.values())\n",
    "\n",
    "        compressor = CohereRerank(top_n=k, user_agent=\"langchain\")\n",
    "        result_documents = compressor.compress_documents(documents=result_documents_unique, query=query)\n",
    "\n",
    "    elif retrieval_method == \"Hybrid_Rerank_Cohere_Compression\":\n",
    "        dense_k = int(rerank_k * dense_percent)\n",
    "        sparse_k = rerank_k - dense_k\n",
    "\n",
    "        bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "        bm25_retriever.k = sparse_k\n",
    "        result_documents_BM25 = bm25_retriever.get_relevant_documents(query)\n",
    "\n",
    "        result_documents_Dense = vectordb.similarity_search(query, k=dense_k)\n",
    "\n",
    "        result_documents_all = []\n",
    "        result_documents_all.extend(result_documents_BM25)\n",
    "        result_documents_all.extend(result_documents_Dense)\n",
    "\n",
    "        unique_documents_dict = {}\n",
    "\n",
    "        for doc in result_documents_all:\n",
    "            doc.metadata[\"original_text\"] = doc.page_content\n",
    "            if doc.page_content not in unique_documents_dict:\n",
    "                unique_documents_dict[doc.page_content] = doc\n",
    "\n",
    "        # Extracting the unique documents from the dictionary\n",
    "        result_documents_unique = list(unique_documents_dict.values())\n",
    "\n",
    "        compressor_1 = CohereRerank(top_n=k, user_agent=\"langchain\")\n",
    "        result_documents_Cohere = compressor.compressor_1(documents=result_documents_unique, query=query)\n",
    "\n",
    "        llm = OpenAI(temperature=0)\n",
    "        compressor_2 = LLMChainExtractor.from_llm(llm)\n",
    "        result_documents = compressor_2.compress_documents(documents=result_documents_Cohere, query=query)\n",
    "\n",
    "    elif retrieval_method == \"Rerank_Contriever\":\n",
    "        retrieved_documents = vectordb.similarity_search(query, k=50)     \n",
    "        tokenizer = AutoTokenizer.from_pretrained('facebook/contriever-msmarco')\n",
    "        model = AutoModel.from_pretrained('facebook/contriever-msmarco')\n",
    "        model.to(device)\n",
    "\n",
    "        texts = [query]\n",
    "        for doc in retrieved_documents:\n",
    "            texts.append(doc.page_content)\n",
    "\n",
    "        # Process texts in batches and compute embeddings\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), 16):\n",
    "            batch_texts = texts[i:i+16]\n",
    "            inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "            with torch.no_grad():  # Inference mode\n",
    "                outputs = model(**inputs)\n",
    "            batch_embeddings = mean_pooling(outputs[0], inputs['attention_mask'])\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "        # Calculate score from contriever, embeddings[0] is the embedding of the query\n",
    "        scores_and_documents = [(embeddings[0] @ embedding, doc) for doc, embedding in zip(retrieved_documents, embeddings[1:])]\n",
    "        # Order docs\n",
    "        sorted_documents = sorted(scores_and_documents, key=lambda x: x[0].item(), reverse=True)\n",
    "        # Only retrieve top k documents\n",
    "        result_documents = [doc for _, doc in sorted_documents[:k]]\n",
    "\n",
    "    elif retrieval_method == \"Hybrid_Rerank_Contriever\":\n",
    "\n",
    "        bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "        bm25_retriever.k = 25\n",
    "        result_documents_BM25 = bm25_retriever.get_relevant_documents(query)\n",
    "\n",
    "        result_documents_Dense = vectordb.similarity_search(query, k=25)\n",
    "\n",
    "        result_documents_all = []\n",
    "        result_documents_all.extend(result_documents_BM25)\n",
    "        result_documents_all.extend(result_documents_Dense)\n",
    "\n",
    "        unique_documents_dict = {}\n",
    "\n",
    "        for doc in result_documents_all:\n",
    "            if doc.page_content not in unique_documents_dict:\n",
    "                unique_documents_dict[doc.page_content] = doc\n",
    "\n",
    "        # Extracting the unique documents from the dictionary\n",
    "        result_documents_unique = list(unique_documents_dict.values())\n",
    "                        \n",
    "        tokenizer = AutoTokenizer.from_pretrained('facebook/contriever-msmarco')\n",
    "        model = AutoModel.from_pretrained('facebook/contriever-msmarco').to(device)\n",
    "\n",
    "        texts = [query]\n",
    "        for doc in result_documents_unique:\n",
    "            texts.append(doc.page_content)\n",
    "\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), 16):\n",
    "            torch.cuda.empty_cache()\n",
    "            batch_texts = texts[i:i+16]\n",
    "            inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "            with torch.no_grad():  # Inference mode\n",
    "                outputs = model(**inputs)\n",
    "            batch_embeddings = mean_pooling(outputs[0], inputs['attention_mask'])\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        \n",
    "\n",
    "        # Calculate score from contriever, embeddings[0] is the embedding of the query\n",
    "        scores_and_documents = [(embeddings[0] @ embedding, doc) for doc, embedding in zip(result_documents_unique, embeddings[1:])]\n",
    "        # Order docs\n",
    "        sorted_documents = sorted(scores_and_documents, key=lambda x: x[0].item(), reverse=True)\n",
    "        # Only retrieve top k documents\n",
    "        result_documents = [doc for _, doc in sorted_documents[:k]]\n",
    "\n",
    "    # Uses dense retrieval and Ms marco cross encoder for reranking (always retrieve 50 documents and then return the top k after reranking)\n",
    "    elif retrieval_method == \"Rerank_Cross_Encoder_Ms_Marco\":\n",
    "        retrieved_documents = vectordb.similarity_search(query, k=50)     \n",
    "        inputs = [(doc.page_content, query) for doc in retrieved_documents]\n",
    "\n",
    "        model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=512)\n",
    "        scores = model.predict(inputs, batch_size=16)\n",
    "\n",
    "        # Associate scores with documents\n",
    "        scores_and_documents = list(zip(scores, result_documents_unique))   \n",
    "        # Sort the documents based on scores\n",
    "        sorted_documents = sorted(scores_and_documents, key=lambda x: x[0], reverse=True)\n",
    "        # Retrieve top k documents\n",
    "        result_documents = [doc for _, doc in sorted_documents[:k]]\n",
    "\n",
    "    elif retrieval_method == \"Hybrid_Rerank_Cross_Encoder_Ms_Marco\":\n",
    "\n",
    "        bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "        bm25_retriever.k = 25\n",
    "        result_documents_BM25 = bm25_retriever.get_relevant_documents(query)\n",
    "\n",
    "        result_documents_Dense = vectordb.similarity_search(query, k=25)\n",
    "\n",
    "        result_documents_all = []\n",
    "        result_documents_all.extend(result_documents_BM25)\n",
    "        result_documents_all.extend(result_documents_Dense)\n",
    "\n",
    "        unique_documents_dict = {}\n",
    "\n",
    "        for doc in result_documents_all:\n",
    "            if doc.page_content not in unique_documents_dict:\n",
    "                unique_documents_dict[doc.page_content] = doc\n",
    "\n",
    "        # Extracting the unique documents from the dictionary\n",
    "        result_documents_unique = list(unique_documents_dict.values())\n",
    "        inputs = [(doc.page_content, query) for doc in result_documents_unique]\n",
    "\n",
    "        model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=512)\n",
    "        scores = model.predict(inputs, batch_size=16)\n",
    "\n",
    "        # Associate scores with documents\n",
    "        scores_and_documents = list(zip(scores, result_documents_unique))   \n",
    "        # Sort the documents based on scores\n",
    "        sorted_documents = sorted(scores_and_documents, key=lambda x: x[0], reverse=True)\n",
    "        # Retrieve top k documents\n",
    "        result_documents = [doc for _, doc in sorted_documents[:k]]\n",
    "\n",
    "    # Uses dense retrieval and BGE cross encode for reranking (always retrieve 50 documents and then return the top k after reranking)\n",
    "    elif retrieval_method == \"Rerank_Cross_Encoder_BGE\":\n",
    "        retrieved_documents = vectordb.similarity_search(query, k=50)   \n",
    "        inputs = [(doc.page_content, query) for doc in retrieved_documents]\n",
    "\n",
    "        reranker = FlagReranker('BAAI/bge-reranker-base', use_fp16=False)\n",
    "        scores = reranker.compute_score(inputs, batch_size=16)\n",
    "\n",
    "        # Associate scores with documents\n",
    "        scores_and_documents = list(zip(scores, result_documents_unique))   \n",
    "        # Sort the documents based on scores\n",
    "        sorted_documents = sorted(scores_and_documents, key=lambda x: x[0], reverse=True)\n",
    "        # Retrieve top k documents\n",
    "        result_documents = [doc for _, doc in sorted_documents[:k]]\n",
    "\n",
    "    elif retrieval_method == \"Hybrid_Rerank_Cross_Encoder_BGE\":\n",
    "        bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "        bm25_retriever.k = 25\n",
    "        result_documents_BM25 = bm25_retriever.get_relevant_documents(query)\n",
    "\n",
    "        result_documents_Dense = vectordb.similarity_search(query, k=25)\n",
    "\n",
    "        result_documents_all = []\n",
    "        result_documents_all.extend(result_documents_BM25)\n",
    "        result_documents_all.extend(result_documents_Dense)\n",
    "\n",
    "        unique_documents_dict = {}\n",
    "\n",
    "        for doc in result_documents_all:\n",
    "            if doc.page_content not in unique_documents_dict:\n",
    "                unique_documents_dict[doc.page_content] = doc\n",
    "\n",
    "        # Extracting the unique documents from the dictionary\n",
    "        result_documents_unique = list(unique_documents_dict.values())\n",
    "        inputs = [(doc.page_content, query) for doc in result_documents_unique]\n",
    "\n",
    "        reranker = FlagReranker('BAAI/bge-reranker-base', use_fp16=False)\n",
    "        scores = reranker.compute_score(inputs, batch_size=16)\n",
    "\n",
    "        # Associate scores with documents\n",
    "        scores_and_documents = list(zip(scores, result_documents_unique))   \n",
    "        # Sort the documents based on scores\n",
    "        sorted_documents = sorted(scores_and_documents, key=lambda x: x[0], reverse=True)\n",
    "        # Retrieve top k documents\n",
    "        result_documents = [doc for _, doc in sorted_documents[:k]]\n",
    "\n",
    "    elif retrieval_method == \"HyDE\":\n",
    "        if hyde_document is None:\n",
    "            hyde_document = generate_hyde_doc(query)\n",
    "        result_documents = vectordb.similarity_search(hyde_document.page_content, k=k)\n",
    "\n",
    "    elif retrieval_method == \"Hybrid_HyDE\":\n",
    "        if hyde_document is None:\n",
    "            hyde_document = generate_hyde_doc(query)\n",
    "\n",
    "        bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "        bm25_retriever.k = k\n",
    "        vectordb_retriever = vectordb.as_retriever(search_kwargs={\"k\": k}, search_type=\"similarity\")\n",
    "        ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, vectordb_retriever], weights=[0.5, 0.5])\n",
    "        result_documents = ensemble_retriever.get_relevant_documents(hyde_document.page_content)\n",
    "        result_documents = result_documents[:k]\n",
    "        \n",
    "    elif retrieval_method == \"MMR\":\n",
    "        result_documents = vectordb.max_marginal_relevance_search(query=query, k=k, fetch_k=50)\n",
    "        \n",
    "    elif retrieval_method == \"Hybrid_MMR\":\n",
    "\n",
    "        vectordb_retriever = vectordb.as_retriever(search_kwargs={\"k\": 25}, search_type=\"similarity\")\n",
    "        result_documents_MRR = vectordb.max_marginal_relevance_search(query=query, k=25, fetch_k=50)\n",
    "        \n",
    "        bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "        bm25_retriever.k = 25\n",
    "        result_documents_BM25 = bm25_retriever.get_relevant_documents(query)\n",
    "\n",
    "        result_documents = []\n",
    "        result_documents.append(result_documents_MRR)\n",
    "        result_documents.append(result_documents_BM25)\n",
    "\n",
    "        ensemble_retriever = EnsembleRetriever(retrievers=[vectordb_retriever, bm25_retriever], weights=[0.5, 0.5])\n",
    "        result_documents = ensemble_retriever.weighted_reciprocal_rank(result_documents)\n",
    "        result_documents = result_documents[:k]\n",
    "\n",
    "    elif retrieval_method == \"Hybrid_MMR_Cohere\":\n",
    "\n",
    "        vectordb_retriever = vectordb.as_retriever(search_kwargs={\"k\": 25}, search_type=\"similarity\")\n",
    "        result_documents_MRR = vectordb.max_marginal_relevance_search(query=query, k=25, fetch_k=50)\n",
    "        \n",
    "        bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "        bm25_retriever.k = 25\n",
    "        result_documents_BM25 = bm25_retriever.get_relevant_documents(query)\n",
    "\n",
    "        result_documents_all = []\n",
    "        result_documents_all.extend(result_documents_MRR)\n",
    "        result_documents_all.extend(result_documents_BM25)\n",
    "\n",
    "        unique_documents_dict = {}\n",
    "\n",
    "        for doc in result_documents_all:\n",
    "            if doc.page_content not in unique_documents_dict:\n",
    "                unique_documents_dict[doc.page_content] = doc\n",
    "\n",
    "        # Extracting the unique documents from the dictionary\n",
    "        result_documents_unique = list(unique_documents_dict.values())\n",
    "\n",
    "        compressor = CohereRerank(top_n=k, user_agent=\"langchain\")\n",
    "        result_documents = compressor.compress_documents(documents=result_documents_unique, query=query)\n",
    "\n",
    "    elif retrieval_method == \"Parent_Child\":\n",
    "        fs = LocalFileStore(os.environ.get(\"PARENT_DOC_PATH\") + f\"\\\\{vectordb._collection.name}\")\n",
    "        store = create_kv_docstore(fs)\n",
    "        parent_child_retriever = MultiVectorRetriever(vectorstore=vectordb, docstore=store, id_key=\"parent_id\", search_kwargs={\"k\": k})\n",
    "        result_documents = parent_child_retriever.get_relevant_documents(query=query)\n",
    "\n",
    "    elif retrieval_method == \"Hybrid_Parent_Child\":\n",
    "        fs_dense = LocalFileStore(os.environ.get(\"PARENT_DOC_PATH\") + f\"\\\\{vectordb._collection.name}\")\n",
    "        store_dense = create_kv_docstore(fs_dense)  \n",
    "\n",
    "        fs_sparse= LocalFileStore(os.environ.get(\"PARENT_DOC_PATH\") + f\"\\\\{document_file_name}\")\n",
    "        store_sparse = create_kv_docstore(fs_sparse)  \n",
    "\n",
    "        result_documents = []\n",
    "       \n",
    "        bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "        bm25_retriever.k = 25\n",
    "        result_documents_BM25 = bm25_retriever.get_relevant_documents(query)\n",
    "\n",
    "        ids = []\n",
    "        for d in result_documents_BM25:\n",
    "            if d.metadata[\"parent_id\"] not in ids:\n",
    "                ids.append(d.metadata[\"parent_id\"])\n",
    "        docs = store_sparse.mget(ids)\n",
    "        result_documents_sparse = [d for d in docs if d is not None]\n",
    "        result_documents.append(result_documents_sparse)\n",
    "\n",
    "        parent_child_retriever_dense = MultiVectorRetriever(vectorstore=vectordb, docstore=store_dense, id_key=\"parent_id\", search_kwargs={\"k\": 25})\n",
    "        result_documents_dense = parent_child_retriever_dense.get_relevant_documents(query=query)\n",
    "        result_documents.append(result_documents_dense)\n",
    "\n",
    "        ensemble_retriever = EnsembleRetriever(retrievers=[parent_child_retriever_dense, bm25_retriever], weights=[0.5, 0.5])\n",
    "        result_documents = ensemble_retriever.weighted_reciprocal_rank(result_documents)\n",
    "        result_documents = result_documents[:k]\n",
    "\n",
    "    elif retrieval_method == \"Hybrid_Parent_Child_Cohere\":\n",
    "        fs_dense = LocalFileStore(os.environ.get(\"PARENT_DOC_PATH\") + f\"\\\\{vectordb._collection.name}\")\n",
    "        store_dense = create_kv_docstore(fs_dense)  \n",
    "\n",
    "        fs_sparse= LocalFileStore(os.environ.get(\"PARENT_DOC_PATH\") + f\"\\\\{document_file_name}\")\n",
    "        store_sparse = create_kv_docstore(fs_sparse)  \n",
    "       \n",
    "        bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "        bm25_retriever.k = 25\n",
    "        result_documents_BM25 = bm25_retriever.get_relevant_documents(query)\n",
    "        ids = []\n",
    "        for d in result_documents_BM25:\n",
    "            if d.metadata[\"parent_id\"] not in ids:\n",
    "                ids.append(d.metadata[\"parent_id\"])\n",
    "        docs = store_sparse.mget(ids)\n",
    "        result_documents_sparse = [d for d in docs if d is not None]\n",
    "\n",
    "        parent_child_retriever_dense = MultiVectorRetriever(vectorstore=vectordb, docstore=store_dense, id_key=\"parent_id\", search_kwargs={\"k\": 25})\n",
    "        result_documents_dense = parent_child_retriever_dense.get_relevant_documents(query=query)\n",
    "\n",
    "        result_documents_all = []\n",
    "        result_documents_all.extend(result_documents_sparse)\n",
    "        result_documents_all.extend(result_documents_dense)\n",
    "\n",
    "        unique_documents_dict = {}\n",
    "\n",
    "        for doc in result_documents_all:\n",
    "            if doc.page_content not in unique_documents_dict:\n",
    "                unique_documents_dict[doc.page_content] = doc\n",
    "\n",
    "        # Extracting the unique documents from the dictionary\n",
    "        result_documents_unique = list(unique_documents_dict.values())\n",
    "\n",
    "        compressor = CohereRerank(top_n=k, user_agent=\"langchain\")\n",
    "        result_documents = compressor.compress_documents(documents=result_documents_unique, query=query)\n",
    "\n",
    "    elif retrieval_method == \"Hybrid_Parent_Child_MMR_Cohere\":\n",
    "        fs_dense = LocalFileStore(os.environ.get(\"PARENT_DOC_PATH\") + f\"\\\\{vectordb._collection.name}\")\n",
    "        store_dense = create_kv_docstore(fs_dense)  \n",
    "\n",
    "        fs_sparse= LocalFileStore(os.environ.get(\"PARENT_DOC_PATH\") + f\"\\\\{document_file_name}\")\n",
    "        store_sparse = create_kv_docstore(fs_sparse)  \n",
    "       \n",
    "        bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "        bm25_retriever.k = 13\n",
    "        result_documents_BM25 = bm25_retriever.get_relevant_documents(query)\n",
    "        parent_child_retriever_sparse = MultiVectorRetriever(sparse_documents=result_documents_BM25, docstore=store_sparse, id_key=\"parent_id\", search_kwargs={\"k\": 13})\n",
    "        result_documents_sparse = parent_child_retriever_sparse.get_relevant_documents(query=query)\n",
    "\n",
    "        parent_child_retriever_dense = MultiVectorRetriever(vectorstore=vectordb, docstore=store_dense, id_key=\"parent_id\", search_kwargs={\"k\": 13})\n",
    "        result_documents_dense = parent_child_retriever_dense.get_relevant_documents(query=query)\n",
    "\n",
    "\n",
    "        result_documents_MRR = vectordb.max_marginal_relevance_search(query=query, k=13, fetch_k=50)\n",
    "        \n",
    "        bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "        bm25_retriever.k = 13\n",
    "        result_documents_BM25 = bm25_retriever.get_relevant_documents(query)\n",
    "\n",
    "        result_documents_all = []\n",
    "        result_documents_all.extend(result_documents_MRR)\n",
    "        result_documents_all.extend(result_documents_BM25)\n",
    "        result_documents_all.extend(result_documents_sparse)\n",
    "        result_documents_all.extend(result_documents_dense)\n",
    "\n",
    "        unique_documents_dict = {}\n",
    "\n",
    "        for doc in result_documents_all:\n",
    "            if doc.page_content not in unique_documents_dict:\n",
    "                unique_documents_dict[doc.page_content] = doc\n",
    "\n",
    "        # Extracting the unique documents from the dictionary\n",
    "        result_documents_unique = list(unique_documents_dict.values())\n",
    "        print(\"length unique: \", len(result_documents_unique))\n",
    "\n",
    "        compressor = CohereRerank(top_n=k, user_agent=\"langchain\")\n",
    "        result_documents = compressor.compress_documents(documents=result_documents_unique, query=query)\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    # If the generation does not happen in this method, add a constant of an average time taken to generate multi query / HyDE\n",
    "    if (retrieval_method == \"Multi_Query\" and len(multiple_queries) > 0) or (retrieval_method == \"Rerank_Multi_Query\" and len(multiple_queries)> 0) or (retrieval_method == \"Hybrid_Multi_Query\" and len(multiple_queries) > 0):\n",
    "        duration += 2\n",
    "    if (retrieval_method == \"HyDE\" and hyde_document is not None) or (retrieval_method == \"Hybrid_HyDE\" and hyde_document is not None):\n",
    "        duration += 2.5\n",
    "\n",
    "    return result_documents, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from typing import Any\n",
    "from typing import Tuple\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "from mistralai.client import MistralClient\n",
    "\n",
    "\n",
    "def prompt_LLM(contexts: List[Document], prompt_version: int, llm, prompt: str, model_provider: str) -> Tuple[str, float, str]:\n",
    "    \"\"\"\n",
    "    Generates the answers to a prompt with the provided contexts by using the llm and prompt template.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Check if the query is aimed at a template and check if the context document also have a template\n",
    "    # If it is a template question the query and system prompt has to be altered\n",
    "    # Only check first two because otherwise the re-ranked score is not high enough to assume that the retrieved template is valid for that question\n",
    "    is_template_question = False\n",
    "    template_path = \"\"\n",
    "    if \"template\" in prompt.lower():\n",
    "        for context in contexts[:2]:\n",
    "            if \"template_path\" in context.metadata:\n",
    "                is_template_question = True\n",
    "                template_path = context.metadata[\"template_path\"]\n",
    "                break\n",
    "\n",
    "    if is_template_question:\n",
    "        concatenated_contexts = \"\"\n",
    "        for index, document in enumerate(contexts[:2], start=1):\n",
    "            original_text = document.metadata.get(\"original_text\", \"\")\n",
    "            concatenated_contexts += f\"{index}. {original_text}\\n\\n\"\n",
    "    else:\n",
    "        concatenated_contexts = \"\"\n",
    "        for index, document in enumerate(contexts, start=1):\n",
    "            original_text = document.metadata.get(\"original_text\", \"\")\n",
    "            concatenated_contexts += f\"{index}. {original_text}\\n\\n\"\n",
    "\n",
    "    # Check if question mark is at the end of the prompt\n",
    "    if not prompt.endswith(\"?\"):\n",
    "        prompt = prompt + \"?\"\n",
    "\n",
    "    # Concatenate the sources of all contexts\n",
    "    concatenated_sources = \"\"\n",
    "    for index, document in enumerate(contexts, start=1):\n",
    "        source = document.metadata.get(\"source\", \"\")\n",
    "        concatenated_sources += f\"{index}. {source}\\n\\n\"\n",
    "\n",
    "    # When using GPT\n",
    "    if model_provider == \"OpenAI\":\n",
    "        if is_template_question and prompt_version == 1:\n",
    "            system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
    "                f\"\"\"Answer the following question with that you can provide a template to the user and explicitly name {template_path} as the path to the file. After that end your answer.\"\"\"\n",
    "            )\n",
    "            human_message_prompt = HumanMessagePromptTemplate.from_template(\"\"\"Question: {question} \\n Context: {context}\"\"\")\n",
    "            chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "            answer = llm(chat_prompt.format_prompt(context=concatenated_contexts, question=prompt).to_messages()).content\n",
    "\n",
    "            answer\n",
    "\n",
    "        elif is_template_question and prompt_version == 2:\n",
    "            system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
    "                f\"\"\"Answer the following question with that you can provide a template to the user and explicitly name {template_path} as the path to the file. After that end your answer.\"\"\"\n",
    "            )\n",
    "            human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "                \"\"\"Question: I need a template for the backup policy inside ISO 27001. Can you provide me with that?\n",
    "                Context: \n",
    "                1. You can find a possible template for the backup policy from the Annex A of ISO 27001 in the local file system under: './../../inputData/Templates/template_files/processed/Backup policy.docx'. It contains pre-written texts for purpose, scope, content and more for the backup policy.\n",
    "\n",
    "                2. ISO 27001 compliance checklist: ISO 27001 is the global gold standard for ensuring the security of information and its supporting assets. Obtaining ISO 27001 certification can help an organization prove its security practices to potential customers anywhere in the world. Our ISO 27001 checklist: 1 Develop a roadmap for successful implementation of an ISMS and ISO 27001 certification Implement Plan, Do, Check, Act (PDCA) process to recognize challenges and identify gaps for remediation Consider ISO 27001 certification costs relative to org size and number of employees Clearly define scope of work to plan certification time to completion Select an ISO 27001 auditor 2 Set the scope of your organization's ISMS Decide which business areas are covered by the ISMS and which are out of scope Consider additional security controls for business processes that are required to pass ISMS-protected information across the trust boundary Inform stakeholders regarding scope of the ISMS 3 Establish an ISMS governing body Build a governance team with management oversight Incorporate key members of top management, e.g. senior leadership and executive management with responsibility for strategy and resource allocation 4 Conduct an inventory of information assets Consider all assets where information is stored, processed, and accessible\n",
    "\n",
    "                Answer: You can find a possible template for the backup policy from the Annex A of ISO 27001 in the local file system under: './../../inputData/Templates/template_files/processed/Backup policy.docx'. It contains pre-written texts for purpose, scope, content and more for the backup policy.\n",
    "\n",
    "                Question: I need a template for the change management policy inside ISO 27001. Can you provide me with that?\n",
    "                Context:\n",
    "                1. What Are ISO 27001 Annex A Controls? **Set by the International Organization for Standardization (ISO) and the > International Electrotechnical Commission (IEC), ISO/IEC 27001 Annex A > defines the 14 categories with a toal of 114 information security controls an organization can address to > receive and maintain its ISO 27001 certification. ** ISO 27001 defines and audits these controls during stage two of the ISO 27001 certification process. An external accredited certification body runs a series of evidentiary audits that confirm the organization's technology and processes are correctly deployed and working properly. The auditors also confirm the implemented solutions align with the controls that were declared to be in use by the organization during part one, the documentation review stage of the certification process. Since industry compliance requirements, technology needs, and scope of operations are unique for each organization, the ISO 27001 Annex A control list serves as a framework, rather than a checklist of requirements. For the certification, however, each firm must draft a Statement of Applicability (SoA), defining the specific Annex A controls based on the company's identified risks, legal and contractual requirements, and overall business needs.\n",
    "\n",
    "                2. You can find a possible template for the change management policy from the Annex A of ISO 27001 in the local file system under: './../../inputData/Templates/template_files/processed/Change management policy.docx\n",
    "\n",
    "                Answer: You can find a possible template for the change management policy from the Annex A of ISO 27001 in the local file system under: './../../inputData/Templates/template_files/processed/Change management policy.docx\n",
    "                \n",
    "                Question: {question}\n",
    "                Context: {context}\n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "            chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "            answer = llm(chat_prompt.format_prompt(context=concatenated_contexts, question=prompt).to_messages()).content\n",
    "\n",
    "            answer\n",
    "\n",
    "        elif prompt_version == 1:\n",
    "            system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
    "                \"\"\"You are an expert in information security, especially for ISO 27001 certifications. Answer the following question as truthfully as possible, using the provided context and not prior knowledge. If the answer is not contained within the context or the question is not related to the topic of information security or ISO 27001, respond with 'I am sorry. I do not have knowledge on that topic'. Write a maximum of 200 words.\"\"\"\n",
    "            )\n",
    "            human_message_prompt = HumanMessagePromptTemplate.from_template(\"\"\"Question: {question} \\n Context: {context}\"\"\")\n",
    "            chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "            answer = llm(chat_prompt.format_prompt(context=concatenated_contexts, question=prompt).to_messages()).content\n",
    "\n",
    "            answer\n",
    "\n",
    "        elif prompt_version == 2:\n",
    "            system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
    "                \"\"\"You are an expert in information security, especially for ISO 27001 certifications. Answer the following question as truthfully as possible, using the provided context and not prior knowledge. If the answer is not contained within the context or the question is not related to the topic of information security or ISO 27001, respond with 'I am sorry. I do not have knowledge on that topic'. Write a maximum of 200 words.\"\"\"\n",
    "            )\n",
    "            human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "                \"\"\"Question: What does the term \"asset\" mean in ISO-27001 and what requirement does the standard have regarding the identification and inventory of information assets?\n",
    "                Context: \n",
    "                1. asset valuation the whole risk assessment process should consider the organisation's context and the needs and expectations of interested parties. Furthermore, ISO 27001 provides no guidance as to the basis on which control selection decisions should be made, other than to say that they should be selected \"taking account of the risk assessment results\" (6.1.3 a), which will necessarily take into account how the organisation prioritises the risks for treatment (6.1.2). Finally, the ISO 27001 management system clauses have no requirement in terms of your methodology for the identification or valuation of information assets. How you value an asset in an asset-based methodology is, however, going to be fundamental to how much you will be prepared to invest in protecting it. ISO 27000 doesn't offer a definition for an asset, although it is reasonable to simply define it as 'anything that has value to the organisation'. The organisation's fixed-asset register is unlikely to provide practical help in this regard: many critical assets may already (through application of the financial depreciation policy, or of the accounting convention that assets should be shown on the balance sheet at the /ower of historic cost — less depreciation — or current market value) have been written down below their actual useful value to the organisation. Many other, even more critical, assets (such as brand value, key supplier and customer contracts, staff know-how, intellectual property and databases) may not even be on\n",
    "\n",
    "                2. or home office. - Information about systems in a demilitarized zone, as part of a security/network concept. Such lists or plans are a good starting point for an inventory list according to A-5.9. It is worth noting that not all data is contained in a single directory. Often, it is spread across multiple subdirectories that are under the responsibility of different entities. The ISO 27002 explicitly allows this, but it requires clear organizational guidelines to avoid duplicate or inconsistent entries, different names, etc. In practice, due to expected challenges, it is often concluded that it is better to have a single central inventory list of information assets. If existing (older) inventories of the organization are discovered, it is necessary to check whether they meet all the requirements described in A-5.9 before incorporating them into the ISMS. If necessary, updates or revisions should be made. For each asset, the inventory list should include at least the following information: a unique identifier, asset owner, and asset location. Before explaining these three data fields, it is advisable to maintain the inventory list not only because the standard requires it but also as a basis for further work in the ISMS. use: As part of the risk assessment (ISMS-6.1), information about security objectives, specific risks, and references to action lists may be listed for each asset in the directory. e Similarly, for certain types of assets, information about suppliers, procurement costs, maintenance work\n",
    "\n",
    "                3. the guidelines and any related asset labelling scheme developed to meet the requirements of control A.8.2.2 (Labelling of information) or similar controls. Are vendors assets? We identified one of the classes of information assets as \"Services on which computer systems depend: computing and communications services, and general utilities such as heating, lighting, power and air-conditioning\". This gives rise to a simple question: are the suppliers/vendors of these essential services also assets? There are two practical answers to this question. The best solution is probably to use a mix of the two, but in doing so *\" See, specifically, chapter 9 of IT Governance: An International Guide to Data Security and ISO27001/TSO27002, Alan Calder and Steve G Watkins (Kogan Page, 2019). 101 8: Information assets it is essential that the exact approach to be used in each specific case is determined by a common set of rules. These should be defined in the risk assessment documentation. One option is to decide that the vendor itself is not an asset — the organisation that is within the scope of the ISMS does not own the vendors — but that the services provided by the vendor and, possibly, the relationship with the vendor are both assets within the scope of the ISMS. The logic behind this option is that a relationship with a vendor can be an asset if it is a key supplier in terms of the information aspects of whatever it is they supply. For example, a stationery supplier would not, we suggest, be a key relationship\n",
    "\n",
    "                Answer: The term \"asset\" in ISO-27001 refers to anything that holds value for an organization. This includes properties, buildings, machinery, facilities, business processes, as well as information assets such as data, systems, and IT services. One requirement of the standard is that all relevant information assets must be identified and inventoried. This is typically done by recording information such as asset location, classification and the asset owner in a table or database. Inventorying can be facilitated by grouping similar assets or implementing a hierarchy.\n",
    "\n",
    "                Question: What is the purpose of information security policies (A.5) in an organization and how are they defined?\n",
    "                Context:\n",
    "                1. What is an information security policy?** An information security policy, often referred to as an _infosec policy_ , is a set of regulations carefully designed to govern the access, use and retention of critical business information. These policies implement a robust framework of processes and tools to ensure absolute protection against unauthorised access, thereby safeguarding an organisation's sensitive information assets. Information security policies follow a common structure and format. They include:  * A statement describing the types of activities covered by the policy  * A statement of commitment issued by management, providing evidence that management has assigned sufficient resources to support ongoing compliance with the policy  * A number of specific responsibilities for employees regarding their use and protection of organisational data. Note that most organisations should aim to employ a data protection officer, whose role it is to maintain and implement these changes as well as add solutions to data protection problems. ## **What is Annex A.5?** This Annex describes the concepts, requirements and recommendations related to information security policies. The purpose of this Annex is to describe the concepts, requirements and recommendations related to information security policies. It covers policy definition, implementation and review. In addition to providing guidance on the implementation of information security policies, Annex A.5 also addresses how to report on\n",
    "\n",
    "                2. though both are against organisational policy), that could be considered inconsistent enforcement.  * **Integrity:** When assigning system permissions, have the system users got minimum viable access rights, or do they have permissions that could compromise the integrity of the system unnecessarily? ## **What is the objective of Annex A.5?** The purpose of information security policies is to help protect an organisation's assets and operations from risks associated with cybersecurity. They are meant to be flexible enough to cover different types of systems and their vulnerabilities, as well as multiple modes of operation, such as traditional and cloud-based operations. Information security policies are the documents that define the standards for information security within an organisation. They can be formal or informal. This Annex describes how to develop an information security policy and how to implement it in your organisation. ## **What are the Annex A.5 information security policy controls?** ### **A.5.1.1 Policies for information security** According to ISO 27001, all organisations must conduct themselves in a transparent manner with their stakeholders. To protect their data, all stakeholders must be informed of the policies in place within the organisation. Policies play a critical role throughout the whole information security process. Therefore, any policies created by the business must first be examined, authorised, and then communicated to employees and third parties. They must also be\n",
    "\n",
    "                3. within the organisation. Policies play a critical role throughout the whole information security process. Therefore, any policies created by the business must first be examined, authorised, and then communicated to employees and third parties. They must also be included in the A.7 human resource security control, and they must be adhered to by all employees. ### **A.5.1.2 Review of the policies for information security** To keep updated with any changes, whether internal or external, the organisation's ISMS policies must be updated on a regular basis. Management changes, governing laws, industry standards, and technology are examples of these developments. The documentation should always represent standards and procedures to preserve the confidentiality, integrity, and availability of files, and an information security breach may result in policy change and improvement. ## **Why is information security policy important for your organisation's information security management?** An information security policy helps your organisation classify your organisations' sensitive data. This depends in part on applicable regulations, but it should also take into account any external factors that could affect risk perception, such as industry competition or geopolitical climate change. Information classifications can range from low (confidential) through medium (secret), high (top secret), even top secret plus or beyond top secret. The exact terms used may vary slightly depending on which agency or company\n",
    "\n",
    "                Answer: Information security policies (A.5) hold great importance in an organization. They serve to depict the overall direction of the organization regarding information security and establish goals and strategies to achieve these objectives. These policies contain fundamental rules and procedures that are applicable within the organization. In addition to a security policy, there are often topic-specific security policies targeting specific audiences, which describe the applicable security rules and measures for a particular subject. Examples of such policies include workplace security practices, virus/malware protection, email security, and access control. The organization is free to create and implement relevant policies.\n",
    "                \n",
    "                Question: {question}\n",
    "                Context: {context}\n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "            chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "            answer = llm(chat_prompt.format_prompt(context=concatenated_contexts, question=prompt).to_messages()).content\n",
    "\n",
    "            answer\n",
    "\n",
    "    # When using LLama\n",
    "    elif model_provider == \"Replicate\":\n",
    "        if is_template_question:\n",
    "            system_prompt = f\"Answer the following question with that you can provide a template to the user and explicitly name {template_path} as the path to the file. After that end your answer.\"\n",
    "            prompt = f\"\"\"\\\n",
    "            [INST]  Question: {prompt} \\n Context: {concatenated_contexts} [/INST]\n",
    "            \"\"\"\n",
    "            answer = llm(system_prompt=system_prompt, prompt=prompt, max_new_tokens=200, temperature=0.01)\n",
    "            answer\n",
    "\n",
    "        elif prompt_version == 1:\n",
    "            system_prompt = \"You are an expert in information security, especially for ISO 27001 certifications. Answer the following question as truthfully as possible, using the provided context and not prior knowledge. If the answer is not contained within the context or the question is not related to the topic of information security or ISO 27001, respond with 'I am sorry. I do not have knowledge on that topic'. Write a maximum of 200 words.\"\n",
    "            prompt = f\"\"\"\\\n",
    "            [INST]  Question: {prompt} \\n Context: {concatenated_contexts}[/INST]\n",
    "            \"\"\"\n",
    "            answer = llm(system_prompt=system_prompt, prompt=prompt, max_new_tokens=200, temperature=0.01)\n",
    "            answer\n",
    "\n",
    "    # When using HuggingFace\n",
    "    # Prompt style resembles t5 style\n",
    "    elif model_provider == \"HuggingFace\":\n",
    "        if is_template_question:\n",
    "            prompt_ = f\"\"\"\\\n",
    "            Answer the following question with that you can provide a template to the user and explicitly name {template_path} as the path to the file. After that end your answer. \\n\n",
    "           Question: {prompt} \\n Context: {concatenated_contexts} \\n Answer:\n",
    "            \"\"\"\n",
    "            llm.tokenizer.model_max_length = 2048\n",
    "            answer = llm(prompt_, min_length=100, max_length=250)\n",
    "            answer = answer[0][\"generated_text\"]\n",
    "\n",
    "        elif prompt_version == 1:\n",
    "            prompt_ = f\"\"\"\\\n",
    "            You are an expert in information security, especially for ISO 27001 certifications. Answer the following question as truthfully as possible, using the provided context. Write at least 100 words. \\n\n",
    "           Question: {prompt} \\n Context: {concatenated_contexts} \\n Answer:\n",
    "            \"\"\"\n",
    "            llm.tokenizer.model_max_length = 2048\n",
    "            answer = llm(prompt_, min_length=100, max_length=210)\n",
    "            answer = answer[0][\"generated_text\"]\n",
    "\n",
    "    elif model_provider == \"Mistral\":\n",
    "        if is_template_question:\n",
    "\n",
    "            sys_message = f\"Answer the following question with that you can provide a template to the user and explicitly name {template_path} as the path to the file. After that end your answer.\"\n",
    "            query_ = f\"Question: {prompt} \\n Context: {concatenated_contexts}\"\n",
    "            prompt_ = f\"<s> [INST] {sys_message} [/INST] \\n User: {query_} \\n Answer: \"\n",
    "\n",
    "            api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "            llm = MistralClient(api_key=api_key)\n",
    "            messages = [ChatMessage(role=\"user\", content=prompt_)]\n",
    "\n",
    "            response = llm.chat(model=\"mistral-small\", messages=messages, temperature=0, max_tokens=210)\n",
    "            answer = response.choices[0].message.content\n",
    "\n",
    "        elif prompt_version == 1:\n",
    "\n",
    "            sys_message = f\"\"\"\\\n",
    "           \"You are an expert in information security, especially for ISO 27001 certifications. Answer the following question as truthfully as possible, using the provided context and not prior knowledge. If the answer is not contained within the context or the question is not related to the topic of information security or ISO 27001, respond with 'I am sorry. I do not have knowledge on that topic'. Write a maximum of 200 words.\"\"\"\n",
    "            query_ = f\"Question: {prompt} \\n Context: {concatenated_contexts}\"\n",
    "            prompt_ = f\"<s> [INST] {sys_message} [/INST] \\n User: {query_} \\n Answer: \"\n",
    "\n",
    "            api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "            llm = MistralClient(api_key=api_key)\n",
    "            messages = [ChatMessage(role=\"user\", content=prompt_)]\n",
    "\n",
    "            response = llm.chat(model=\"open-mixtral-8x7b\", messages=messages, temperature=0, max_tokens=260)\n",
    "            answer = response.choices[0].message.content\n",
    "    else:\n",
    "        raise Exception(\"Error, raised exception: Wrong modelProvider provided.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    return answer, duration, concatenated_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from typing import Any\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def prompt_LLM_only(prompt_version: int, llm, prompt: str, model_provider: str) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Generates the answers to a prompt by using the llm and prompt template.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Check if question mark is at the end of the prompt\n",
    "    if not prompt.endswith(\"?\"):\n",
    "        prompt = prompt + \"?\"\n",
    "\n",
    "    # When using GPT\n",
    "    if model_provider == \"OpenAI\":\n",
    "        if prompt_version == 1:\n",
    "            system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
    "                \"\"\"You are an expert in information security, especially for ISO 27001 certifications. Answer the following question with a maximum of 200 words. If you do not know the answer, respond with 'I am sorry. I do not have knowledge on that topic'.\"\"\"\n",
    "            )\n",
    "            human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "                \"\"\"Question: {question}\"\"\")\n",
    "            chat_prompt = ChatPromptTemplate.from_messages(\n",
    "                [system_message_prompt, human_message_prompt])\n",
    "\n",
    "            answer = llm(chat_prompt.format_prompt(question=prompt).to_messages()).content\n",
    "\n",
    "    # When using LLama\n",
    "    elif model_provider == \"Replicate\":\n",
    "        if prompt_version == 1:\n",
    "            system_prompt = \"You are an expert in information security, especially for ISO 27001 certifications. Answer the following question with a maximum of 200 words.\"\n",
    "            prompt_ = f\"\"\"\\\n",
    "            [INST]  Question: {prompt} [/INST]\n",
    "            \"\"\"\n",
    "            answer = llm(system_prompt=system_prompt, prompt=prompt_,max_new_tokens=200, temperature=0.01)\n",
    "\n",
    "    # When using HuggingFace\n",
    "    elif model_provider == \"HuggingFace\":\n",
    "        if prompt_version == 1:\n",
    "            prompt_ = f\"\"\"\\\n",
    "           You are an expert in information security, especially for ISO 27001 certifications. Answer the following question with a maximum of 200 words. \\n\n",
    "           Question: {prompt} \\n Answer:\n",
    "            \"\"\"\n",
    "            answer = llm(prompt_, min_length=100, max_length=250)\n",
    "            answer = answer[0][\"generated_text\"]\n",
    "\n",
    "    elif model_provider == \"Mistral\":\n",
    "        if prompt_version == 1:\n",
    "            sys_message = f\"\"\"\\\n",
    "           You are an expert in information security, especially for ISO 27001 certifications. Answer the following question with a maximum of 200 words. \\n\n",
    "           Question: {prompt}\n",
    "            \"\"\"\n",
    "            query_ = f\"Question: {prompt}\"\n",
    "            prompt_ = f'<s> [INST] {sys_message} [/INST] \\n User: {query_} \\n Answer: '\n",
    "            \n",
    "            api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "            llm = MistralClient(api_key=api_key)\n",
    "            messages = [\n",
    "                ChatMessage(role=\"user\", content=prompt_)\n",
    "            ]\n",
    "            \n",
    "            response = llm.chat(model=\"mistral-small\", messages=messages, temperature=0, max_tokens=370)\n",
    "            answer = response.choices[0].message.content\n",
    "\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"Error, raised exception: Wrong modelProvider provided.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    return answer, duration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
